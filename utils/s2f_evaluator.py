'''
==============================================================================
SF2F Evaluator: Speech-to-Face Performance Evaluation Module
==============================================================================

Paper: "Speech2Face: Learning a Face from a Voice" 
URL: https://arxiv.org/abs/2006.05888

ì´ íŒŒì¼ì€ SF2F(Speech Fusion to Face) ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì „ìš© í‰ê°€ê¸°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
training_utils.py, test.pyì™€ í˜‘ë ¥í•˜ì—¬ ì¢…í•©ì ì¸ í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì„±í•©ë‹ˆë‹¤.

SF2F ë…¼ë¬¸ì˜ í•µì‹¬ í‰ê°€ ë©”íŠ¸ë¦­ë“¤ (Core Evaluation Metrics):
=============================================================================

ğŸ“ 1. Identity Recall@K (ë…¼ë¬¸ Table 2ì˜ í•µì‹¬ ì§€í‘œ)
   - Recall@1, @5, @10: ìŒì„±ì—ì„œ ì˜¬ë°”ë¥¸ ì–¼êµ´ì„ ì°¾ëŠ” ì •í™•ë„
   - SF2Fì˜ ê°€ì¥ ì¤‘ìš”í•œ ì„±ëŠ¥ ì§€í‘œë¡œ, ìŒì„±-ì–¼êµ´ ë§¤í•‘ì˜ ì •í™•ì„±ì„ ì§ì ‘ ì¸¡ì •

ğŸ“ 2. VGGFace Feature Space Similarity (ë…¼ë¬¸ Section 4.1)
   - L1/L2 Distance: ì–¼êµ´ íŠ¹ì§• ê³µê°„ì—ì„œì˜ ê±°ë¦¬ ì¸¡ì •
   - Cosine Similarity: íŠ¹ì§• ë²¡í„° ê°„ì˜ ë°©í–¥ì„± ìœ ì‚¬ë„
   - í”½ì…€ ë‹¨ìœ„ê°€ ì•„ë‹Œ ê³ ìˆ˜ì¤€ ì–¼êµ´ íŠ¹ì§•ì—ì„œì˜ í‰ê°€

ğŸ“ 3. Inter-Human Similarity Analysis (ë…¼ë¬¸ì˜ í˜ì‹ ì  í‰ê°€ ì ‘ê·¼)
   - ìƒì„±ëœ ì–¼êµ´ë“¤ ê°„ì˜ ë‹¤ì–‘ì„±ê³¼ í˜„ì‹¤ì„± í‰ê°€
   - ê³¼ë„í•œ ìœ ì‚¬ì„±(mode collapse) ë°©ì§€ í™•ì¸

ğŸ“ 4. Multi-Modal Face Generation (ë…¼ë¬¸ Section 3ì˜ êµ¬í˜„ ê²€ì¦)
   - ë‹¤ì–‘í•œ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì¼ê´€ëœ ì–¼êµ´ ìƒì„± ëŠ¥ë ¥ í‰ê°€
   - ìŒì„±ì˜ ì‹œê°„ì  ë³€í™”ì— ëŒ€í•œ ì–¼êµ´ ìƒì„±ì˜ ì•ˆì •ì„± ì¸¡ì •

Technical Implementation:
=============================================================================
- FaceNet (VGGFace2 pretrained): ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸
- VGG Face Cropping: í‘œì¤€í™”ëœ ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ [0.235, 0.195, 0.765, 0.915]
- Multiple Query Methods: L1, L2, Cosine Similarity ê¸°ë°˜ ê²€ìƒ‰
- Batch Processing: ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ íš¨ìœ¨ì  ì²˜ë¦¬

Evaluation Modes:
=============================================================================
1. naive: ê¸°ë³¸ì ì¸ ë°°ì¹˜ë³„ í‰ê°€
2. average_facenet_embedding: ì—¬ëŸ¬ ì–¼êµ´ ì´ë¯¸ì§€ì˜ FaceNet ì„ë² ë”© í‰ê· í™”
3. average_voice_embedding: ìŒì„± ì„ë² ë”© í‰ê· í™”ë¥¼ í†µí•œ ì•ˆì •ì  ì–¼êµ´ ìƒì„±

ì´ í‰ê°€ê¸°ëŠ” SF2F ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ëª¨ë“  ì •ëŸ‰ì  í‰ê°€ ì§€í‘œë¥¼ êµ¬í˜„í•˜ì—¬
ìŒì„±-ì–¼êµ´ ë§¤í•‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
'''


import os
import json
import math
from collections import defaultdict
import time
from copy import deepcopy

import numpy as np
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from scipy.stats import entropy
from torchvision.models import inception_v3
from pyprind import prog_bar
from tensorflow import gfile
from imageio import imwrite

from datasets import fast_imagenet_deprocess_batch, imagenet_deprocess_batch
import datasets
import models
from models import InceptionResnetV1, fixed_image_standardization


# ===================================================================
# SF2F ë…¼ë¬¸ Figure 3: VGG Face Detectionì˜ í‘œì¤€ ê²½ê³„ ìƒì
# ë…¼ë¬¸ì—ì„œ ì–¼êµ´ ì˜ì—­ í‘œì¤€í™”ë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê¸°ì¤€ ì¢Œí‘œ
# ===================================================================
# left, top, right, bottom - ì–¼êµ´ ì˜ì—­ì˜ ìƒëŒ€ì  ì¢Œí‘œ
# SF2Fì—ì„œ ì¼ê´€ëœ ì–¼êµ´ ë¹„êµë¥¼ ìœ„í•´ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì´ ì˜ì—­ìœ¼ë¡œ í¬ë¡­
VGG_BOX = [0.235, 0.195, 0.765, 0.915]


class S2fEvaluator:
    """
    ===================================================================
    SF2F ì „ìš© í‰ê°€ê¸° í´ë˜ìŠ¤ (Speech-to-Face Evaluator)
    ===================================================================
    
    SF2F ë…¼ë¬¸ì˜ í•µì‹¬ í‰ê°€ ë©”íŠ¸ë¦­ë“¤ì„ êµ¬í˜„í•˜ëŠ” ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ:
    
    ğŸ“ ë…¼ë¬¸ Table 2: Identity Recall@K ë©”íŠ¸ë¦­
    ğŸ“ ë…¼ë¬¸ Section 4.1: VGGFace íŠ¹ì§• ê³µê°„ ìœ ì‚¬ë„ ë¶„ì„
    ğŸ“ ë…¼ë¬¸ì˜ í˜ì‹ ì  í‰ê°€: Inter-Human Similarity ì¸¡ì •
    ğŸ“ ë…¼ë¬¸ Section 3: ë‹¤ì¤‘ ëª¨ë‹¬ ì–¼êµ´ ìƒì„± í‰ê°€
    
    ì´ í´ë˜ìŠ¤ëŠ” ìŒì„±-ì–¼êµ´ ë§¤í•‘ì˜ ì •í™•ì„±ê³¼ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    """
    
    def __init__(self,
                 loader,
                 options,
                 nframe_range=None,
                 extraction_size=100,
                 hq_emb_dict=True,
                 face_gen_mode='naive',
                 facenet_return_pooling=False):
        '''
        ===================================================================
        SF2F í‰ê°€ê¸° ì´ˆê¸°í™”
        ===================================================================
        
        Args:
            loader: ë°ì´í„° ë¡œë” (VoxCeleb ìŒì„±-ì–¼êµ´ ìŒ ë°ì´í„°)
            options: ì„¤ì • ì˜µì…˜ë“¤ (ì´ë¯¸ì§€ ì •ê·œí™”, í¬ë¡­ ì„¤ì • ë“±)
            extraction_size: í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„° í¬ê¸° (ë…¼ë¬¸ì—ì„œ 100, 200, 300 ì‚¬ìš©)
            hq_emb_dict: ê³ í’ˆì§ˆ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© ì—¬ë¶€
            face_gen_mode: ì–¼êµ´ ìƒì„± ëª¨ë“œ
                - 'naive': ê¸°ë³¸ ë°°ì¹˜ë³„ ìƒì„±
                - 'average_facenet_embedding': FaceNet ì„ë² ë”© í‰ê· í™”
                - 'average_voice_embedding': ìŒì„± ì„ë² ë”© í‰ê· í™”
            facenet_return_pooling: FaceNet í’€ë§ ë ˆì´ì–´ ë°˜í™˜ ì—¬ë¶€
        
        ğŸ“ ë…¼ë¬¸ í•µì‹¬: VGGFace2ë¡œ ì‚¬ì „í›ˆë ¨ëœ FaceNetì„ íŠ¹ì§• ì¶”ì¶œê¸°ë¡œ ì‚¬ìš©
        ì´ëŠ” SF2F ë…¼ë¬¸ì—ì„œ ì–¼êµ´ í’ˆì§ˆ í‰ê°€ì˜ ê¸°ì¤€ìœ¼ë¡œ í™œìš©
        '''
        self.loader = deepcopy(loader)
        # Fuser í‰ê°€ë¥¼ ìœ„í•´ ë™ì¼í•œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ê·¸ë£¹ ì‚¬ìš© ë³´ì¥
        # SF2Fì—ì„œ ì¼ê´€ëœ ìŒì„± íŠ¹ì§• ì‚¬ìš©ì„ ìœ„í•œ ì„¤ì •
        self.loader.dataset.shuffle_mel_segments = False
        if nframe_range is not None:
            self.loader.dataset.nframe_range = nframe_range
            
        # ğŸ“ ë…¼ë¬¸ í•µì‹¬: VGGFace2 ì‚¬ì „í›ˆë ¨ëœ FaceNet ì‚¬ìš©
        # SF2F ë…¼ë¬¸ Section 4.1ì—ì„œ ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œì˜ í‘œì¤€ìœ¼ë¡œ ì±„íƒ
        # ì´ëŠ” ì–¼êµ´ ì¸ì‹ì—ì„œ state-of-the-art ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸
        self.facenet = InceptionResnetV1(
            pretrained='vggface2',
            auto_input_resize=True,
            return_pooling=facenet_return_pooling).cuda().eval()
        self.float_dtype = torch.cuda.FloatTensor
        self.long_dtype = torch.cuda.LongTensor
        self.options = options
        
        # ğŸ“ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ë¯¸ì§€ ì •ê·œí™” ë°©ë²•
        # SF2Fì—ì„œ ì¼ê´€ëœ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ í‘œì¤€í™”
        self.image_normalize_method= \
                self.options["data"]["data_opts"]["image_normalize_method"]
        
        # ğŸ“ ë…¼ë¬¸ Figure 3: FaceNet ì…ë ¥ì„ ìœ„í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        # ImageNet ì •ê·œí™” í•´ì œ â†’ FaceNet í‘œì¤€í™” ì ìš©
        self.do_deprocess_and_preprocess = \
            self.options["eval"]["facenet"]["deprocess_and_preprocess"]
            
        # ğŸ“ SF2F ë…¼ë¬¸ì˜ í•µì‹¬: VGG í‰ê·  ê²½ê³„ ìƒìë¡œ ì–¼êµ´ í¬ë¡­
        # í‘œì¤€í™”ëœ ì–¼êµ´ ì˜ì—­ì—ì„œë§Œ ë¹„êµí•˜ì—¬ ì •í™•í•œ í‰ê°€ ë³´ì¥
        self.crop_faces = \
            self.options["eval"]["facenet"]["crop_faces"]
        self.extraction_size = extraction_size
        self.hq_emb_dict = hq_emb_dict
        self.face_gen_mode = face_gen_mode

        # ğŸ“ ë…¼ë¬¸ í•µì‹¬: ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì–¼êµ´ì— ëŒ€í•œ FaceNet ì„ë² ë”© ì‚¬ì „ ê³„ì‚°
        # ì´ëŠ” Recall@K ê³„ì‚°ì„ ìœ„í•œ ê¸°ì¤€ ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        self.get_dataset_embeddings()

        # ===================================================================
        # SF2F ë…¼ë¬¸ Section 4.1: ë‹¤ì–‘í•œ ê±°ë¦¬ ë©”íŠ¸ë¦­ êµ¬í˜„
        # ===================================================================
        # ğŸ“ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” 3ê°€ì§€ ìœ ì‚¬ë„ ì¸¡ì • ë°©ë²•
        # L2 Distance: ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ì¼ë°˜ì ì¸ íŠ¹ì§• ê³µê°„ ê±°ë¦¬)
        self.L2_dist = \
            nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)
        # L1 Distance: ë§¨í•˜íƒ„ ê±°ë¦¬ (robustí•œ ê±°ë¦¬ ì¸¡ì •)
        self.L1_dist = \
            nn.PairwiseDistance(p=1.0, eps=1e-06, keepdim=False)
        # Cosine Similarity: ë²¡í„° ê°„ì˜ ê°ë„ ìœ ì‚¬ë„ (ë°©í–¥ì„± ê³ ë ¤)
        self.cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-08)

    def deprocess_and_preprocess(self, imgs):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸ Figure 3: FaceNet ì…ë ¥ì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        ===================================================================
        
        ì‹¤ì œ/ìƒì„±ëœ ì´ë¯¸ì§€ ë°°ì¹˜ì— ëŒ€í•´ ë‹¤ìŒ ê³¼ì •ì„ ìˆ˜í–‰:
        1. ImageNet ì •ê·œí™” í•´ì œ (ì›ë³¸ ì´ë¯¸ì§€ ë³µì›)
        2. FaceNet ì „ìš© í‘œì¤€í™” ì ìš©
        
        ğŸ“ ë…¼ë¬¸ í•µì‹¬: ì¼ê´€ëœ ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ í‘œì¤€í™”ëœ ì „ì²˜ë¦¬
        SF2Fì—ì„œ ëª¨ë“  ì–¼êµ´ ì´ë¯¸ì§€ëŠ” ë™ì¼í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì³ì•¼ í•¨
        '''
        #print('Begin:', imgs[0])  # ë””ë²„ê¹…ìš© ì¶œë ¥
        # ImageNet ì •ê·œí™” í•´ì œ: í›ˆë ¨ ì¤‘ ì‚¬ìš©ëœ ì •ê·œí™”ë¥¼ ë˜ëŒë¦¼
        imgs = fast_imagenet_deprocess_batch(
            imgs,
            normalize_method=self.image_normalize_method)
        #print('Our distribution:', imgs[0])  # ë””ë²„ê¹…ìš© ì¶œë ¥
        
        # ğŸ“ FaceNet ì „ìš© í‘œì¤€í™”: VGGFace2 ì‚¬ì „í›ˆë ¨ì— ë§ëŠ” ì •ê·œí™”
        # ì´ëŠ” ì •í™•í•œ ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•´ í•„ìˆ˜ì 
        imgs = fixed_image_standardization(imgs)
        #print('fixed_image_standardization:', imgs[0])  # ë””ë²„ê¹…ìš© ì¶œë ¥
        return imgs

    def crop_vgg_box(self, imgs):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸: VGG í‰ê·  ê²½ê³„ ìƒìë¥¼ ì‚¬ìš©í•œ í‘œì¤€í™”ëœ ì–¼êµ´ í¬ë¡­
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ í•µì‹¬: ëª¨ë“  ì–¼êµ´ì„ ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ í¬ë¡­í•˜ì—¬ ê³µì •í•œ ë¹„êµ
        VGG_BOX = [0.235, 0.195, 0.765, 0.915] (left, top, right, bottom)
        
        ì´ëŠ” ì–¼êµ´ ì¸ì‹ ë¶„ì•¼ì˜ í‘œì¤€ í¬ë¡­ ì˜ì—­ìœ¼ë¡œ, SF2Fì—ì„œë„ ì±„íƒ
        ë°°ê²½ì´ë‚˜ ë¨¸ë¦¬ì¹´ë½ ë“±ì˜ ì˜í–¥ì„ ìµœì†Œí™”í•˜ê³  ì–¼êµ´ ì˜ì—­ì—ë§Œ ì§‘ì¤‘
        '''
        # VGG í‘œì¤€ ê²½ê³„ ìƒì ì¢Œí‘œ (ìƒëŒ€ì  ë¹„ìœ¨)
        left, top, right, bottom = VGG_BOX
        # = [0.235015, 0.19505739, 0.76817876, 0.9154963]
        N, C, H, W = imgs.shape
        
        # ìƒëŒ€ ì¢Œí‘œë¥¼ ì ˆëŒ€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        left = int(left * W)
        right = int(right * W)
        top = int(top * H)
        bottom = int(bottom * H)
        
        # ì–¼êµ´ ì˜ì—­ë§Œ í¬ë¡­ (í‘œì¤€í™”ëœ ì–¼êµ´ ë¹„êµë¥¼ ìœ„í•´)
        imgs = imgs[:, :, top:bottom+1, left:right+1]
        return imgs

    def get_dataset_embeddings(self):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸: ê¸°ì¤€ ë°ì´í„°ì…‹ì˜ FaceNet ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ í•µì‹¬: Recall@K ê³„ì‚°ì„ ìœ„í•œ Ground Truth ì„ë² ë”© ìƒì„±
        
        ëª¨ë“  ì‹¤ì œ ì–¼êµ´ ì´ë¯¸ì§€ì— ëŒ€í•´:
        1. VGGFace2 ì‚¬ì „í›ˆë ¨ëœ FaceNetìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
        2. í‘œì¤€í™”ëœ ì „ì²˜ë¦¬ ë° í¬ë¡­ ì ìš©
        3. ê³ í’ˆì§ˆ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬ êµ¬ì¶• (hq_emb_dict=Trueì¼ ë•Œ)
        
        ì´ ì„ë² ë”©ë“¤ì€ ìŒì„±ì—ì„œ ìƒì„±ëœ ì–¼êµ´ê³¼ ë¹„êµí•˜ì—¬
        "ì˜¬ë°”ë¥¸ ì‚¬ëŒì„ ì°¾ì•˜ëŠ”ê°€?"ë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.
        '''
        with torch.no_grad():
            # BatchNorm ë ˆì´ì–´ì˜ running_mean/varì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë°©ì§€
            embedding_batches = []
            if self.hq_emb_dict:
                # ğŸ“ ê³ í’ˆì§ˆ ì„ë² ë”© ëª¨ë“œ: ê° IDë³„ ëª¨ë“  ì–¼êµ´ì˜ í‰ê·  ì„ë² ë”© ì‚¬ìš©
                # SF2F ë…¼ë¬¸ì—ì„œ ë” ì•ˆì •ì ì¸ í‰ê°€ë¥¼ ìœ„í•´ ê¶Œì¥í•˜ëŠ” ë°©ì‹
                for i in prog_bar(
                    range(len(self.loader.dataset)),
                    title="[S2fEvaluator: " + \
                        "Preparing FaceNet Embedding Dictionary]",
                    width=50):
                    # ê° IDì˜ ëª¨ë“  ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë“œ
                    ######### unpack the data #########
                    imgs = self.loader.dataset.get_all_faces_of_id(i)
                    imgs = imgs.cuda()
                    ###################################
                    
                    # ğŸ“ SF2F í‘œì¤€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©
                    if self.do_deprocess_and_preprocess:
                        imgs = self.deprocess_and_preprocess(imgs)
                    if self.crop_faces:
                        imgs = self.crop_vgg_box(imgs)
                        
                    # FaceNetìœ¼ë¡œ ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œ
                    embeddings = self.facenet(imgs)
                    # ğŸ“ ë…¼ë¬¸ í•µì‹¬: ì—¬ëŸ¬ ì–¼êµ´ ì´ë¯¸ì§€ì˜ í‰ê·  ì„ë² ë”©ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
                    # ì¡°ëª…, ê°ë„ ë“±ì˜ ë³€í™”ì— robustí•œ ëŒ€í‘œ ì„ë² ë”© ìƒì„±
                    embeddings = torch.mean(embeddings, 0, keepdim=True)
                    embedding_batches.append(embeddings)
            else:
                # ğŸ“ ê¸°ë³¸ ëª¨ë“œ: ë°°ì¹˜ë³„ ë‹¨ì¼ ì„ë² ë”© ì‚¬ìš©
                for batch in prog_bar(
                    self.loader,
                    title="[S2fEvaluator: " + \
                        "Preparing FaceNet Embedding Dictionary]",
                    width=50):
                    # Loop logic
                    ######### unpack the data #########
                    imgs, log_mels, human_ids = batch
                    imgs = imgs.cuda()
                    ###################################
                    if self.do_deprocess_and_preprocess:
                        imgs = self.deprocess_and_preprocess(imgs)
                    if self.crop_faces:
                        imgs = self.crop_vgg_box(imgs)
                    embeddings = self.facenet(imgs)
                    embedding_batches.append(embeddings)
                    
            # ğŸ“ ëª¨ë“  ì„ë² ë”©ì„ í•˜ë‚˜ì˜ í…ì„œë¡œ ê²°í•© (ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„±)
            self.dataset_embedding = torch.cat(embedding_batches, 0)
        print("S2fEvaluator: dataset_embedding shape:",
              self.dataset_embedding.shape)

    def get_pred_img_embeddings(self, model):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸: ìƒì„±ëœ ì–¼êµ´ ì´ë¯¸ì§€ì˜ FaceNet ì„ë² ë”© ì¶”ì¶œ
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ í•µì‹¬: ìŒì„±ì—ì„œ ìƒì„±ëœ ì–¼êµ´ë“¤ì˜ íŠ¹ì§• ë²¡í„° ê³„ì‚°
        
        ì´ í•¨ìˆ˜ëŠ” SF2F ëª¨ë¸ì´ ìŒì„±ì—ì„œ ìƒì„±í•œ ì–¼êµ´ë“¤ì— ëŒ€í•´:
        1. ë™ì¼í•œ FaceNetìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
        2. ì‹¤ì œ ì–¼êµ´ê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©
        3. 3ê°€ì§€ ìƒì„± ëª¨ë“œ ì§€ì›:
           - naive: ê¸°ë³¸ ë°°ì¹˜ë³„ ìƒì„±
           - average_facenet_embedding: ìƒì„±ëœ ì—¬ëŸ¬ ì–¼êµ´ì˜ í‰ê· 
           - average_voice_embedding: ìŒì„± ì„ë² ë”© í‰ê· í™” í™œìš©
        
        ì´ ì„ë² ë”©ë“¤ì€ dataset_embeddingê³¼ ë¹„êµë˜ì–´ Recall@Kë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        '''
        training_status = model.training
        model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (ë“œë¡­ì•„ì›ƒ ë¹„í™œì„±í™”)
        pred_img_embedding_batches = []
        
        with torch.no_grad():
            # BatchNorm ë ˆì´ì–´ì˜ running_mean/varì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë°©ì§€
            if self.face_gen_mode == 'naive':
                # ğŸ“ ê¸°ë³¸ ëª¨ë“œ: ë°°ì¹˜ë³„ ì–¼êµ´ ìƒì„± ë° í‰ê°€
                for batch in prog_bar(
                    self.loader,
                    title="[S2fEvaluator: " + \
                        "Getting FaceNet Embedding for Predicted Images]",
                    width=50):
                    ######### unpack the data #########
                    imgs, log_mels, human_ids = batch
                    imgs = imgs.cuda()
                    log_mels = log_mels.type(self.float_dtype)
                    human_ids = human_ids.type(self.long_dtype)
                    ###################################
                    
                    # ğŸ“ SF2F ëª¨ë¸ ì‹¤í–‰: ìŒì„± â†’ ì–¼êµ´ ìƒì„±
                    # í›ˆë ¨ ì¤‘ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ì‹¤í–‰
                    model_out = model(log_mels)
                    imgs_pred, others = model_out
                    
                    # Multi-Resolution ì¶œë ¥ ì²˜ë¦¬ (ê°€ì¥ ë†’ì€ í•´ìƒë„ ì‚¬ìš©)
                    if isinstance(imgs_pred, tuple):
                        imgs_pred = imgs_pred[-1]
                        
                    # ğŸ“ ì‹¤ì œ ì–¼êµ´ê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš© (ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´)
                    if self.do_deprocess_and_preprocess:
                        imgs_pred = self.deprocess_and_preprocess(imgs_pred)
                    if self.crop_faces:
                        imgs_pred = self.crop_vgg_box(imgs_pred)
                        
                    # FaceNetìœ¼ë¡œ ìƒì„±ëœ ì–¼êµ´ì˜ íŠ¹ì§• ì¶”ì¶œ
                    pred_img_embeddings = self.facenet(imgs_pred)
                    pred_img_embedding_batches.append(pred_img_embeddings)
                    
            elif self.face_gen_mode == 'average_facenet_embedding':
                # ğŸ“ ê³ ê¸‰ ëª¨ë“œ: ì—¬ëŸ¬ ìƒì„± ì–¼êµ´ì˜ FaceNet ì„ë² ë”© í‰ê· í™”
                # SF2Fì—ì„œ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©
                for i in prog_bar(
                    range(len(self.loader.dataset)),
                    title="[S2fEvaluator: " + \
                        "Getting FaceNet Embedding for Predicted Images, " + \
                            "with average Facenet embedding policy]",
                    width=50):
                    ######### unpack the data #########
                    # ê° IDì˜ ëª¨ë“  ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ë¡œë“œ
                    log_mels = self.loader.dataset.get_all_mel_segments_of_id(i)
                    log_mels = log_mels.type(self.float_dtype)
                    ###################################
                    
                    # ì—¬ëŸ¬ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì–¼êµ´ ìƒì„±
                    model_out = model(log_mels)
                    imgs_pred, others = model_out
                    if isinstance(imgs_pred, tuple):
                        imgs_pred = imgs_pred[-1]
                        
                    if self.do_deprocess_and_preprocess:
                        imgs_pred = self.deprocess_and_preprocess(imgs_pred)
                    if self.crop_faces:
                        imgs_pred = self.crop_vgg_box(imgs_pred)
                        
                    pred_img_embeddings = self.facenet(imgs_pred)
                    # ğŸ“ í•µì‹¬: ì—¬ëŸ¬ ìƒì„± ì–¼êµ´ì˜ í‰ê·  ì„ë² ë”©ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
                    # ìŒì„±ì˜ ì‹œê°„ì  ë³€í™”ì— robustí•œ ëŒ€í‘œ ì„ë² ë”© ìƒì„±
                    pred_img_embeddings = torch.mean(
                        pred_img_embeddings, 0, keepdim=True)
                    pred_img_embedding_batches.append(pred_img_embeddings)
                    
            elif self.face_gen_mode == 'average_voice_embedding':
                # ğŸ“ ìµœê³ ê¸‰ ëª¨ë“œ: ìŒì„± ì„ë² ë”© í‰ê· í™”ë¥¼ í†µí•œ ì•ˆì •ì  ì–¼êµ´ ìƒì„±
                # SF2Fì—ì„œ ì œì•ˆí•˜ëŠ” ê°€ì¥ ì •êµí•œ í‰ê°€ ë°©ì‹
                for i in prog_bar(
                    range(len(self.loader.dataset)),
                    title="[S2fEvaluator: " + \
                        "Getting FaceNet Embedding for Predicted Images, " + \
                            "with average voice embedding policy]",
                    width=50):
                    ######### unpack the data #########
                    log_mels = self.loader.dataset.get_all_mel_segments_of_id(i)
                    log_mels = log_mels.type(self.float_dtype)
                    ###################################
                    
                    # ğŸ“ SF2F í˜ì‹ : ìŒì„± ì„ë² ë”© í‰ê· í™”ë¡œ ë” ì•ˆì •ì ì¸ ì–¼êµ´ ìƒì„±
                    model_out = model(log_mels, average_voice_embedding=True)
                    imgs_pred, others = model_out
                    if isinstance(imgs_pred, tuple):
                        imgs_pred = imgs_pred[-1]
                        
                    if self.do_deprocess_and_preprocess:
                        imgs_pred = self.deprocess_and_preprocess(imgs_pred)
                    if self.crop_faces:
                        imgs_pred = self.crop_vgg_box(imgs_pred)
                        
                    pred_img_embeddings = self.facenet(imgs_pred)
                    pred_img_embedding_batches.append(pred_img_embeddings)
                    
            # ëª¨ë“  ìƒì„±ëœ ì–¼êµ´ì˜ ì„ë² ë”©ì„ ê²°í•©
            pred_img_embedding = torch.cat(pred_img_embedding_batches, 0)
            
        model.train(mode=training_status)  # ì›ë˜ í›ˆë ¨ ìƒíƒœë¡œ ë³µì›
        return pred_img_embedding

    def L1_query(self, x, y):
        '''
        Given x, extract top K similar features from y, based on L1 distance
        x in shape (N_x, D)
        y in shape (N_y, D)
        '''
        # (N_x, D) --> (N_x, 1, D)
        x = x.unsqueeze(1)
        # Initialize: (N_x, )
        x_ids = torch.tensor(np.arange(x.shape[0])).cpu()
        # (N_y, D) --> (1, N_y, D)
        y = y.unsqueeze(0)
        # Output: (N_x, N_y, D)
        L1_table = torch.abs(x - y)
        # (N_x, N_y, D) --> (N_x, N_y)
        L1_table = torch.mean(L1_table, dim=-1)
        L1_table = torch.neg(L1_table)
        # Top K: (N_x, K)
        top_1_vals, top_1_indices = torch.topk(L1_table, 1, dim=-1)
        top_5_vals, top_5_indices = torch.topk(L1_table, 5, dim=-1)
        top_10_vals, top_10_indices = torch.topk(L1_table, 10, dim=-1)
        top_50_vals, top_50_indices = torch.topk(L1_table, 50, dim=-1)

        recall_at_1 = self.in_top_k(top_1_indices.cpu(), x_ids)
        recall_at_5 = self.in_top_k(top_5_indices.cpu(), x_ids)
        recall_at_10 = self.in_top_k(top_10_indices.cpu(), x_ids)
        recall_at_50 = self.in_top_k(top_50_indices.cpu(), x_ids)

        return recall_at_1, recall_at_5, recall_at_10, recall_at_50

    def cos_query(self, x, y):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸ Table 2: Cosine Similarity ê¸°ë°˜ Recall@K ê³„ì‚°
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ í•µì‹¬: "ìŒì„±ì—ì„œ ì˜¬ë°”ë¥¸ ì–¼êµ´ì„ ì°¾ëŠ” ì •í™•ë„" ì¸¡ì •
        
        Args:
            x: ìƒì„±ëœ ì–¼êµ´ì˜ FaceNet ì„ë² ë”© (N_x, D)
            y: ì‹¤ì œ ì–¼êµ´ì˜ FaceNet ì„ë² ë”© (N_y, D)
            
        Returns:
            recall_tuple: (Recall@1, @2, @5, @10, @20, @50)
            
        ğŸ“ SF2Fì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‰ê°€ ì§€í‘œ:
        - Recall@1: ê°€ì¥ ìœ ì‚¬í•œ 1ê°œê°€ ì •ë‹µì¸ ë¹„ìœ¨ (ì—„ê²©í•œ í‰ê°€)
        - Recall@5: ìƒìœ„ 5ê°œ ì¤‘ ì •ë‹µì´ ìˆëŠ” ë¹„ìœ¨ (ì‹¤ìš©ì  í‰ê°€)  
        - Recall@10: ìƒìœ„ 10ê°œ ì¤‘ ì •ë‹µì´ ìˆëŠ” ë¹„ìœ¨ (ê´€ëŒ€í•œ í‰ê°€)
        
        ì´ëŠ” ìŒì„±-ì–¼êµ´ ë§¤í•‘ì˜ ì •í™•ì„±ì„ ì§ì ‘ ì¸¡ì •í•˜ëŠ” í•µì‹¬ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.
        '''
        # (N_x, D) --> (N_x, 1, D): ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•œ ì°¨ì› í™•ì¥
        x = x.unsqueeze(1)
        # Ground Truth ë ˆì´ë¸” ìƒì„±: ê° ì¿¼ë¦¬ì˜ ì •ë‹µ ì¸ë±ìŠ¤
        x_ids = torch.tensor(np.arange(x.shape[0])).cpu()
        # (N_y, D) --> (1, N_y, D): ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•œ ì°¨ì› í™•ì¥
        y = y.unsqueeze(0)
        
        # ğŸ“ ë…¼ë¬¸ í•µì‹¬: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ë°©í–¥ì„± ê³ ë ¤)
        # Output: (N_x, N_y) - ê° ìƒì„± ì–¼êµ´ê³¼ ëª¨ë“  ì‹¤ì œ ì–¼êµ´ ê°„ì˜ ìœ ì‚¬ë„
        cos_table = self.cos_sim(x, y)

        # ğŸ“ SF2F ë…¼ë¬¸ Table 2: ë‹¤ì–‘í•œ Kê°’ì—ì„œì˜ Top-K ê²€ìƒ‰
        # ê° Kê°’ì€ ì‹¤ì œ ì‘ìš©ì—ì„œì˜ ë‹¤ë¥¸ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜
        top_1_vals, top_1_indices = torch.topk(cos_table, 1, dim=-1)
        top_2_vals, top_2_indices = torch.topk(cos_table, 2, dim=-1)
        top_5_vals, top_5_indices = torch.topk(cos_table, 5, dim=-1)
        top_10_vals, top_10_indices = torch.topk(cos_table, 10, dim=-1)
        top_20_vals, top_20_indices = torch.topk(cos_table, 20, dim=-1)
        top_50_vals, top_50_indices = torch.topk(cos_table, 50, dim=-1)

        # ğŸ“ ê° Kê°’ì—ì„œ ì˜¬ë°”ë¥¸ ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
        recall_at_1 = self.in_top_k(top_1_indices.cpu(), x_ids)
        recall_at_2 = self.in_top_k(top_2_indices.cpu(), x_ids)
        recall_at_5 = self.in_top_k(top_5_indices.cpu(), x_ids)
        recall_at_10 = self.in_top_k(top_10_indices.cpu(), x_ids)
        recall_at_20 = self.in_top_k(top_20_indices.cpu(), x_ids)
        recall_at_50 = self.in_top_k(top_50_indices.cpu(), x_ids)

        recall_tuple = (recall_at_1, recall_at_2, recall_at_5, recall_at_10, \
            recall_at_20, recall_at_50)

        return recall_tuple

    def cal_ih_sim(self, x, y):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸: Inter-Human Similarity Analysis
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ì˜ í˜ì‹ ì  í‰ê°€ ì ‘ê·¼: ìƒì„±ëœ ì–¼êµ´ë“¤ ê°„ì˜ ë‹¤ì–‘ì„± ì¸¡ì •
        
        ê¸°ì¡´ ì–¼êµ´ ì„ë² ë”© ë¶„í¬ì— ëŒ€í•´ ì‚¬ëŒ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì´ëŠ” ë‹¤ìŒì„ í™•ì¸í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤:
        1. Mode Collapse ë°©ì§€: ëª¨ë“  ì–¼êµ´ì´ ë¹„ìŠ·í•˜ê²Œ ìƒì„±ë˜ì§€ ì•ŠëŠ”ê°€?
        2. ë‹¤ì–‘ì„± ë³´ì¡´: ì„œë¡œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ ê³ ìœ í•œ íŠ¹ì§•ì´ ìœ ì§€ë˜ëŠ”ê°€?
        3. í˜„ì‹¤ì„±: ìƒì„±ëœ ì–¼êµ´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ê°€ ì‹¤ì œ ì‚¬ëŒë“¤ê³¼ ë¹„ìŠ·í•œê°€?

        Args:
            x: ì²« ë²ˆì§¸ ì„ë² ë”© ì„¸íŠ¸ (N_x, D)
            y: ë‘ ë²ˆì§¸ ì„ë² ë”© ì„¸íŠ¸ (N_y, D)
            
        Returns:
            ih_sim: í‰ê·  inter-human similarity (0~1, ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±ì´ ë†’ìŒ)
        '''
        # ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•œ ì°¨ì› í™•ì¥
        y = y.unsqueeze(0)  # (1, N_y, D)
        x = x.unsqueeze(1)  # (N_x, 1, D)
        
        # ğŸ“ ëª¨ë“  ìŒ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        # Output: (N_x, N_y) - ëª¨ë“  ê°€ëŠ¥í•œ ìŒ ì¡°í•©ì˜ ìœ ì‚¬ë„
        cos_table = self.cos_sim(x, y)
        cos_table = cos_table.detach().cpu().numpy()
        
        # ğŸ“ ëŒ€ê°ì„  ì œì™¸í•œ ëª¨ë“  ìŒì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„(ëŒ€ê°ì„ )ëŠ” ì œì™¸í•˜ê³  ê³„ì‚°
        ih_sum = 0.0
        for i in range(cos_table.shape[0]):
            for j in range(cos_table.shape[1]):
                if i != j:  # ìê¸° ìì‹  ì œì™¸
                    ih_sum = ih_sum + cos_table[i, j]
                    
        # í‰ê·  inter-human similarity ê³„ì‚°
        ih_sim = ih_sum / float(cos_table.shape[0] * (cos_table.shape[1] - 1))
        return ih_sim

    def in_top_k(self, top_k_indices, gt_labels):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸: Top-K ê²€ìƒ‰ì—ì„œ ì •ë‹µ í¬í•¨ ì—¬ë¶€ í™•ì¸
        ===================================================================
        
        ğŸ“ Recall@Kì˜ í•µì‹¬ ê³„ì‚° ë¡œì§
        
        ê° ì¿¼ë¦¬(ìƒì„±ëœ ì–¼êµ´)ì— ëŒ€í•´ Top-K ê²€ìƒ‰ ê²°ê³¼ì— 
        ì •ë‹µ(ì˜¬ë°”ë¥¸ ì‚¬ëŒì˜ ì‹¤ì œ ì–¼êµ´)ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        
        Args:
            top_k_indices: Top-K ê²€ìƒ‰ ê²°ê³¼ ì¸ë±ìŠ¤ë“¤
            gt_labels: Ground Truth ë ˆì´ë¸” (ì •ë‹µ ì¸ë±ìŠ¤)
            
        Returns:
            recall_rate: ì •ë‹µì´ í¬í•¨ëœ ì¿¼ë¦¬ì˜ ë¹„ìœ¨ (0~1)
        '''
        results = []
        for i, top_k_id in enumerate(top_k_indices):
            gt_label = gt_labels[i]  # ië²ˆì§¸ ì¿¼ë¦¬ì˜ ì •ë‹µ
            
            # ğŸ“ í•µì‹¬: Top-K ê²°ê³¼ì— ì •ë‹µì´ ìˆëŠ”ê°€?
            if gt_label in top_k_id:
                results.append(1.0)  # ì„±ê³µ
            else:
                results.append(0.0)  # ì‹¤íŒ¨
                
        # ì „ì²´ ì¿¼ë¦¬ ì¤‘ ì„±ê³µí•œ ë¹„ìœ¨ ë°˜í™˜
        return np.mean(results)

    def get_metrics(self, model, recall_method='cos_sim', get_ih_sim=False):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸: ì¢…í•©ì ì¸ ìŒì„±-ì–¼êµ´ ë§¤í•‘ ì„±ëŠ¥ í‰ê°€
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ì˜ í•µì‹¬ í‰ê°€ ë©”íŠ¸ë¦­ë“¤ì„ ëª¨ë‘ ê³„ì‚°í•˜ëŠ” í†µí•© í•¨ìˆ˜
        
        SF2F ë…¼ë¬¸ Section 4.1ì—ì„œ ì œì‹œí•œ í‰ê°€ ì§€í‘œë“¤:
        1. Feature Space Distance (L1, L2): íŠ¹ì§• ê³µê°„ì—ì„œì˜ ê±°ë¦¬
        2. Cosine Similarity: íŠ¹ì§• ë²¡í„° ê°„ì˜ ë°©í–¥ì„± ìœ ì‚¬ë„  
        3. Identity Recall@K: ìŒì„±-ì–¼êµ´ ë§¤í•‘ ì •í™•ë„ (ê°€ì¥ ì¤‘ìš”)
        4. Inter-Human Similarity: ë‹¤ì–‘ì„± ë° í˜„ì‹¤ì„± í‰ê°€
        
        Args:
            model: í‰ê°€í•  SF2F ëª¨ë¸
            recall_method: ê²€ìƒ‰ ë°©ë²• ('cos_sim' ë˜ëŠ” 'L1')
            get_ih_sim: Inter-human similarity ê³„ì‚° ì—¬ë¶€
            
        Returns:
            L2_dist, L1_dist, cos_sim, recall_tuple, [ih_sim]
        '''
        # ğŸ“ 1ë‹¨ê³„: ìƒì„±ëœ ì–¼êµ´ë“¤ì˜ FaceNet ì„ë² ë”© ì¶”ì¶œ
        pred_img_embedding = self.get_pred_img_embeddings(model)
        
        # ===================================================================
        # SF2F ë…¼ë¬¸ Section 4.1: Feature Space Similarity Metrics
        # ===================================================================
        # ğŸ“ L2 ê±°ë¦¬: ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ì¼ë°˜ì ì¸ íŠ¹ì§• ê³µê°„ ê±°ë¦¬)
        L2_dist = self.L2_dist(self.dataset_embedding, pred_img_embedding)
        L2_dist = torch.mean(L2_dist).item()
        
        # ğŸ“ L1 ê±°ë¦¬: ë§¨í•˜íƒ„ ê±°ë¦¬ (outlierì— robust)
        L1_dist = self.L1_dist(self.dataset_embedding, pred_img_embedding)
        L1_dist = torch.mean(L1_dist).item()
        
        # ğŸ“ ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë°©í–¥ì„± ê³ ë ¤í•œ ìœ ì‚¬ë„ (ì •ê·œí™”ëœ íŠ¹ì§•ì— ì í•©)
        cos_sim = self.cos_sim(self.dataset_embedding, pred_img_embedding)
        cos_sim = torch.mean(cos_sim).item()

        # ===================================================================
        # SF2F ë…¼ë¬¸ Table 2: Identity Recall@K ê³„ì‚°
        # ===================================================================
        # ğŸ“ í‰ê°€ í¬ê¸° ì„¤ì •: ë…¼ë¬¸ì—ì„œ 100, 200, 300ê°œ ìƒ˜í”Œë¡œ ë‹¤ì–‘í•˜ê²Œ í‰ê°€
        if self.extraction_size is None:
            # ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
            pred_emb_to_use = pred_img_embedding
            data_emb_to_use = self.dataset_embedding
        elif isinstance(self.extraction_size, list):
            # ğŸ“ ë‹¤ì¤‘ í¬ê¸° í‰ê°€: ë” robustí•œ ì„±ëŠ¥ ì¸¡ì •
            # ì˜ˆ: [100, 200, 300] â†’ 100ê°œ, 100-200ê°œ, 200-300ê°œë¡œ ë‚˜ëˆ„ì–´ í‰ê°€
            pred_emb_to_use = [pred_img_embedding[0:self.extraction_size[0]], \
                pred_img_embedding[self.extraction_size[0]:self.extraction_size[1]], \
                pred_img_embedding[self.extraction_size[1]:self.extraction_size[2]]]
            data_emb_to_use = [self.dataset_embedding[0:self.extraction_size[0]], \
                self.dataset_embedding[self.extraction_size[0]:self.extraction_size[1]], \
                self.dataset_embedding[self.extraction_size[1]:self.extraction_size[2]]]
        else:
            # ì§€ì •ëœ í¬ê¸°ë§Œí¼ ì‚¬ìš©
            pred_emb_to_use = pred_img_embedding[0:self.extraction_size]
            data_emb_to_use = self.dataset_embedding[0:self.extraction_size]

        # ğŸ“ ì„ íƒëœ ê²€ìƒ‰ ë°©ë²•ìœ¼ë¡œ Recall@K ê³„ì‚°
        if recall_method == 'L1':
            # L1 ê±°ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
            if isinstance(pred_emb_to_use, list):
                # ë‹¤ì¤‘ í¬ê¸°ì—ì„œ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
                recall_temp = []
                for i, pred_emb in enumerate(pred_emb_to_use):
                    recall_temp.append(self.L1_query(pred_emb, data_emb_to_use[i]))
                recall_tuple = tuple(np.mean(np.array(recall_temp), axis=0))
            else:
                recall_tuple = self.L1_query(pred_emb_to_use, data_emb_to_use)
                
        elif recall_method == 'cos_sim':
            # ğŸ“ SF2F ë…¼ë¬¸ì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
            if isinstance(pred_emb_to_use, list):
                # ë‹¤ì¤‘ í¬ê¸°ì—ì„œ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
                recall_temp = []
                for i, pred_emb in enumerate(pred_emb_to_use):
                    recall_temp.append(self.cos_query(pred_emb, data_emb_to_use[i]))
                recall_tuple = tuple(np.mean(np.array(recall_temp), axis=0))
            else:
                recall_tuple = self.cos_query(pred_emb_to_use, data_emb_to_use)

        # ===================================================================
        # SF2F ë…¼ë¬¸: Inter-Human Similarity Analysis (ì„ íƒì )
        # ===================================================================
        if get_ih_sim:
            # ğŸ“ ìƒì„±ëœ ì–¼êµ´ë“¤ ê°„ì˜ ë‹¤ì–‘ì„± ì¸¡ì •
            ih_sim = self.cal_ih_sim(pred_img_embedding, self.dataset_embedding)
            return L2_dist, L1_dist, cos_sim, recall_tuple, ih_sim
        else:
            return L2_dist, L1_dist, cos_sim, recall_tuple

    def get_faces_from_different_segments(self, model, output_dir):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸ Section 3: Multi-Modal Face Generation Analysis
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ í•µì‹¬: ë‹¤ì–‘í•œ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì¼ê´€ëœ ì–¼êµ´ ìƒì„± ëŠ¥ë ¥ í‰ê°€
        
        ì´ í•¨ìˆ˜ëŠ” SF2F ëª¨ë¸ì˜ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤:
        1. Temporal Consistency: ê°™ì€ ì‚¬ëŒì˜ ë‹¤ë¥¸ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì¼ê´€ëœ ì–¼êµ´ ìƒì„±
        2. Voice-Face Stability: ìŒì„±ì˜ ì‹œê°„ì  ë³€í™”ì— robustí•œ ì–¼êµ´ ë§¤í•‘
        3. Identity Preservation: ë‹¤ì–‘í•œ ë°œí™”ì—ì„œë„ ë™ì¼í•œ ì‹ ì› ìœ ì§€
        
        ğŸ“ SF2F ë…¼ë¬¸ì—ì„œ ê°•ì¡°í•˜ëŠ” í•µì‹¬ í‰ê°€ ìš”ì†Œ:
        - ìŒì„±ì˜ prosody, í†¤, ì†ë„ ë³€í™”ì—ë„ ì•ˆì •ì ì¸ ì–¼êµ´ ìƒì„±
        - ê°™ì€ ì‚¬ëŒì´ì§€ë§Œ ë‹¤ë¥¸ ê°ì •/ìƒí™©ì˜ ìŒì„±ì—ì„œë„ ì¼ê´€ì„± ìœ ì§€
        - ì‹œê°„ì— ë”°ë¥¸ ìŒì„± ë³€í™”ì— ëŒ€í•œ ëª¨ë¸ì˜ robustness ê²€ì¦
        
        Args:
            model: í‰ê°€í•  SF2F ëª¨ë¸
            output_dir: ê²°ê³¼ ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
            
        Output Structure:
            output_dir/
            â”œâ”€â”€ 0/
            â”‚   â”œâ”€â”€ origin_0.png, origin_1.png, ... (ì‹¤ì œ ì–¼êµ´ë“¤)
            â”‚   â””â”€â”€ pred_0.png, pred_1.png, ...   (ìƒì„±ëœ ì–¼êµ´ë“¤)
            â”œâ”€â”€ 1/
            â”‚   â””â”€â”€ ...
            â””â”€â”€ ...
        '''
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        temp_loader = deepcopy(self.loader)
        
        # ğŸ“ ê° ì‹ ì›(ID)ë³„ë¡œ ë‹¤ì¤‘ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
        for i in prog_bar(
            range(len(self.loader.dataset)),
            title="[S2fEvaluator: " + \
                "Generating Faces from Different Speech Segments]",
            width=50):
            ######### unpack the data #########
            # ğŸ“ í•œ ì‚¬ëŒì˜ ëª¨ë“  ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë“œ (Ground Truth)
            imgs = temp_loader.dataset.get_all_faces_of_id(i)
            # ğŸ“ í•œ ì‚¬ëŒì˜ ëª¨ë“  ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ë¡œë“œ (ë‹¤ì–‘í•œ ë°œí™”)
            # SF2F í•µì‹¬: ì„œë¡œ ë‹¤ë¥¸ ìŒì„±ì—ì„œ ë™ì¼ ì¸ë¬¼ ì–¼êµ´ ìƒì„± ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸
            log_mels = temp_loader.dataset.get_all_mel_segments_of_id(i)
            imgs = imgs.cuda()
            log_mels = log_mels.type(self.float_dtype)
            ###################################
            
            # ğŸ“ SF2F ëª¨ë¸ë¡œ ì—¬ëŸ¬ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì–¼êµ´ ìƒì„±
            # ê°™ì€ ì‚¬ëŒì˜ ë‹¤ë¥¸ ìŒì„±ë“¤ â†’ ì¼ê´€ëœ ì–¼êµ´ë“¤ì´ ìƒì„±ë˜ëŠ”ê°€?
            with torch.no_grad():
                model_out = model(log_mels)
            imgs_pred, _ = model_out
            
            # Multi-Resolution ì¶œë ¥ ì²˜ë¦¬ (ê°€ì¥ ë†’ì€ í•´ìƒë„ ì‚¬ìš©)
            if isinstance(imgs_pred, tuple):
                imgs_pred = imgs_pred[-1]
                
            # ğŸ“ ì´ë¯¸ì§€ í›„ì²˜ë¦¬: ì €ì¥ì„ ìœ„í•œ ì •ê·œí™” í•´ì œ
            # ì‹¤ì œ ì–¼êµ´ê³¼ ìƒì„±ëœ ì–¼êµ´ ëª¨ë‘ ë™ì¼í•œ í›„ì²˜ë¦¬ ì ìš©
            imgs = imagenet_deprocess_batch(
                imgs, normalize_method=self.image_normalize_method)
            imgs_pred = imagenet_deprocess_batch(
                imgs_pred, normalize_method=self.image_normalize_method)
            
            # ğŸ“ ê° ì‹ ì›ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            identity_dir = os.path.join(output_dir, str(i))
            gfile.MkDir(identity_dir)

            # ğŸ“ ì‹¤ì œ ì–¼êµ´ ì´ë¯¸ì§€ë“¤ ì €ì¥ (ë¹„êµ ê¸°ì¤€)
            for j in range(imgs.shape[0]):
                img_np = imgs[j].numpy().transpose(1, 2, 0)  # CHW â†’ HWC
                img_path = os.path.join(identity_dir, 'origin_%d.png' % j)
                imwrite(img_path, img_np)

            # ğŸ“ ìƒì„±ëœ ì–¼êµ´ ì´ë¯¸ì§€ë“¤ ì €ì¥ (SF2F ê²°ê³¼)
            # ê° ì´ë¯¸ì§€ëŠ” ì„œë¡œ ë‹¤ë¥¸ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìƒì„±ë¨
            # ë…¼ë¬¸ í‰ê°€: ì´ë“¤ì´ ì–¼ë§ˆë‚˜ ì¼ê´€ë˜ê³  ì‹¤ì œ ì–¼êµ´ê³¼ ìœ ì‚¬í•œê°€?
            for k in range(imgs_pred.shape[0]):
                img_np = imgs_pred[k].numpy().transpose(1, 2, 0)  # CHW â†’ HWC
                img_path = os.path.join(identity_dir, 'pred_%d.png' % k)
                imwrite(img_path, img_np)

    def L2_distances(self, x, y=None):
        '''
        ===================================================================
        SF2F ë…¼ë¬¸ Section 4.1: Feature Space Distance Matrix ê³„ì‚°
        ===================================================================
        
        ğŸ“ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        
        ì´ í•¨ìˆ˜ëŠ” SF2F í‰ê°€ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ í™œìš©ë©ë‹ˆë‹¤:
        1. ìƒì„±ëœ ì–¼êµ´ë“¤ ê°„ì˜ ê±°ë¦¬ ë¶„í¬ ë¶„ì„
        2. ì‹¤ì œ ì–¼êµ´ê³¼ ìƒì„±ëœ ì–¼êµ´ ê°„ì˜ ê±°ë¦¬ ì¸¡ì •
        3. Inter-Human Distance Analysisì˜ ê¸°ë°˜ ê³„ì‚°
        4. Clustering Analysisë¥¼ ìœ„í•œ ê±°ë¦¬ ë§¤íŠ¸ë¦­ìŠ¤ ì œê³µ
        
        Args:
            x: ì²« ë²ˆì§¸ ì„ë² ë”© ì„¸íŠ¸ (Nxd ë§¤íŠ¸ë¦­ìŠ¤)
            y: ë‘ ë²ˆì§¸ ì„ë² ë”© ì„¸íŠ¸ (Mxd ë§¤íŠ¸ë¦­ìŠ¤, ì„ íƒì )
               Noneì´ë©´ x ë‚´ì˜ ëª¨ë“  ìŒ ê°„ ê±°ë¦¬ ê³„ì‚°
               
        Returns:
            dist: NxM ê±°ë¦¬ ë§¤íŠ¸ë¦­ìŠ¤
                  dist[i,j] = ||x[i,:] - y[j,:]||^2 (ì œê³± ìœ í´ë¦¬ë“œ ê±°ë¦¬)
                  
        ğŸ“ SF2Fì—ì„œ í™œìš© ì˜ˆì‹œ:
        - ìƒì„± í’ˆì§ˆ ë¶„ì„: ìƒì„±ëœ ì–¼êµ´ë“¤ì´ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œê°€?
        - ë§¤í•‘ ì •í™•ë„: ìƒì„±ëœ ì–¼êµ´ì´ ì‹¤ì œ ì–¼êµ´ê³¼ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ê°€?
        - í´ëŸ¬ìŠ¤í„°ë§: ë¹„ìŠ·í•œ ìŒì„± íŠ¹ì§•ì„ ê°€ì§„ ì‚¬ëŒë“¤ì˜ ì–¼êµ´ ê·¸ë£¹ ë¶„ì„
        '''
        if y is not None:
            # ğŸ“ ì„œë¡œ ë‹¤ë¥¸ ë‘ ì„¸íŠ¸ ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
            # x: (N, d), y: (M, d) â†’ distances: (N, M)
            differences = x.unsqueeze(1) - y.unsqueeze(0)  # Broadcasting
        else:
            # ğŸ“ ê°™ì€ ì„¸íŠ¸ ë‚´ì˜ ëª¨ë“  ìŒ ê°„ ê±°ë¦¬ ê³„ì‚° (ìê¸° ìì‹  í¬í•¨)
            # x: (N, d) â†’ distances: (N, N)
            differences = x.unsqueeze(1) - x.unsqueeze(0)  # Broadcasting
            
        # ğŸ“ ì œê³± ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°: ||a - b||^2 = sum((a - b)^2)
        # SF2Fì—ì„œ íŠ¹ì§• ê³µê°„ì˜ ê±°ë¦¬ ì¸¡ì •ì— í‘œì¤€ì ìœ¼ë¡œ ì‚¬ìš©
        distances = torch.sum(differences * differences, -1)
        return distances
