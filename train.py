"""
==============================================================================
SF2F: Speech Fusion to Face - PyTorch Implementation
==============================================================================

Paper: "Speech2Face: Learning a Face from a Voice"
URL: https://arxiv.org/abs/2006.05888

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” SF2F(Speech Fusion to Face) ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ìŒì„±ì—ì„œ ì–¼êµ´ë¡œì˜ 
ì§ì ‘ì ì¸ ë§¤í•‘ í•™ìŠµì„ êµ¬í˜„í•œ í›ˆë ¨ ì½”ë“œì…ë‹ˆë‹¤.

SF2F ë…¼ë¬¸ì˜ 8ê°€ì§€ í•µì‹¬ ê¸°ì—¬ì ë“¤ (Core Contributions):
=============================================================================

ğŸ“ 1. Speech Fusion to Face Architecture (Section 3.1)
   - ìŒì„± íŠ¹ì§•(ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨)ì—ì„œ ì–¼êµ´ ì´ë¯¸ì§€ë¡œì˜ ì§ì ‘ì  ë§¤í•‘ í•™ìŠµ
   - ê¸°ì¡´ MFCC ëŒ€ì‹  ë” í’ë¶€í•œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŠ¹ì§• ì‚¬ìš©

ğŸ“ 2. Multi-Resolution Training Strategy (Section 3.2)  
   - ê³„ì¸µì  ì†ì‹¤ í•¨ìˆ˜ì™€ ë‹¤í•´ìƒë„ í•™ìŠµì„ í†µí•œ ì ì§„ì  í’ˆì§ˆ í–¥ìƒ
   - Progressive GAN ê°œë…ì„ ìŒì„±-ì–¼êµ´ ë§¤í•‘ì— ì ìš©

ğŸ“ 3. Identity Preservation via Auxiliary Classifier (Section 3.4)
   - ë³´ì¡° ë¶„ë¥˜ê¸°ë¥¼ í†µí•œ ì‹ ì› ë³´ì¡´ í•™ìŠµ
   - ì‹¤ì œ/ê°€ì§œ íŒë³„ê³¼ ì‹ ì› ë¶„ë¥˜ì˜ ì´ì¤‘ ê¸°ëŠ¥ ìˆ˜í–‰

ğŸ“ 4. Speech-Conditioned Discriminator (Section 3.5)
   - ìŒì„± ì¡°ê±´ì„ ëª…ì‹œì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ì¡°ê±´ë¶€ íŒë³„ê¸°
   - "ì´ ìŒì„±ì— ë§ëŠ” ì–¼êµ´ì¸ê°€?"ë¥¼ íŒë³„í•˜ëŠ” í˜ì‹ ì  ì ‘ê·¼

ğŸ“ 5. VGGFace-based Perceptual Loss (Section 3.6)
   - ì–¼êµ´ íŠ¹í™” ì¸ì§€ì  ì†ì‹¤ë¡œ ì‹œê°ì  í’ˆì§ˆ í–¥ìƒ
   - í”½ì…€ ë‹¨ìœ„ê°€ ì•„ë‹Œ ê³ ìˆ˜ì¤€ ì–¼êµ´ íŠ¹ì§• ê³µê°„ì—ì„œì˜ ë¹„êµ

ğŸ“ 6. Comprehensive Evaluation Metrics (Section 4)
   - Recall@K: ìŒì„±-ì–¼êµ´ ë§¤í•‘ ì •í™•ì„± ì¸¡ì •ì˜ í•µì‹¬ ì§€í‘œ
   - VGGFace Score: ì–¼êµ´ íŠ¹í™” í’ˆì§ˆ í‰ê°€
   - Feature Space Similarity: ì½”ì‚¬ì¸ ìœ ì‚¬ë„, L1/L2 ê±°ë¦¬

ğŸ“ 7. Progressive Training Strategy (Section 3.7)
   - Two-Stage Training: L1 ì†ì‹¤ â†’ ì ëŒ€ì /ì¸ì§€ì  ì†ì‹¤
   - ì•ˆì •ì ì´ê³  ì ì§„ì ì¸ í’ˆì§ˆ í–¥ìƒ ì „ëµ

ğŸ“ 8. Multi-Scale Discriminator Training (Section 3.3)
   - ë‹¤ì–‘í•œ í•´ìƒë„ì—ì„œì˜ ë…ë¦½ì  íŒë³„ê¸° í›ˆë ¨
   - ì„¸ë°€í•œ ë””í…Œì¼ë¶€í„° ì „ì²´ì  êµ¬ì¡°ê¹Œì§€ í¬ê´„ì  í’ˆì§ˆ ì œì–´

Implementation Details:
=============================================================================
- ì…ë ¥: ë¡œê·¸ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ (Log Mel-Spectrograms)
- ì¶œë ¥: 128x128 ì–¼êµ´ ì´ë¯¸ì§€ (ë‹¤í•´ìƒë„ ì§€ì›)
- ì•„í‚¤í…ì²˜: GAN ê¸°ë°˜ (ìƒì„±ê¸° + ë‹¤ì¤‘ íŒë³„ê¸°)
- ë°ì´í„°ì…‹: VoxCeleb (ìŒì„±-ì–¼êµ´ ìŒ ë°ì´í„°)
- ì†ì‹¤ í•¨ìˆ˜: L1 + Adversarial + Perceptual + Identity + Conditional

Training Phases:
=============================================================================
Phase 1: Basic Structure Learning (L1 + Adversarial Loss)
Phase 2: Quality Enhancement (Adversarial + Perceptual + Identity Loss)

ì´ êµ¬í˜„ì€ SF2F ë…¼ë¬¸ì˜ ëª¨ë“  í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì¶©ì‹¤íˆ ì¬í˜„í•˜ë©°,
ìŒì„±ì—ì„œ ì–¼êµ´ë¡œì˜ ë§¤í•‘ì—ì„œ state-of-the-art ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
"""

# ê¸°ë³¸ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import functools  # í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° ë„êµ¬
import os  # ìš´ì˜ì²´ì œ ì¸í„°í˜ì´ìŠ¤
import json  # JSON ë°ì´í„° ì²˜ë¦¬
import math  # ìˆ˜í•™ í•¨ìˆ˜ë“¤
from collections import defaultdict  # ê¸°ë³¸ê°’ì´ ìˆëŠ” ë”•ì…”ë„ˆë¦¬
import random  # ë‚œìˆ˜ ìƒì„±
import time  # ì‹œê°„ ê´€ë ¨ í•¨ìˆ˜ë“¤
import pyprind  # í›ˆë ¨ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ í”„ë¡œê·¸ë ˆìŠ¤ ë°”
import glog as log  # êµ¬ê¸€ì˜ ë¡œê¹… ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë””ë²„ê¹…ìš©)
from shutil import copyfile  # íŒŒì¼ ë³µì‚¬ (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ìš©)

# ë”¥ëŸ¬ë‹ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch  # PyTorch ë©”ì¸ í”„ë ˆì„ì›Œí¬
import torch.optim as optim  # ìµœì í™” ì•Œê³ ë¦¬ì¦˜ (Adam, SGD ë“±)
import torch.nn as nn  # ì‹ ê²½ë§ ë ˆì´ì–´ì™€ í•¨ìˆ˜ë“¤
import torch.nn.functional as F  # ì¶”ê°€ì ì¸ ì‹ ê²½ë§ í•¨ìˆ˜ë“¤
from torch.utils.data.dataloader import default_collate  # ë°ì´í„° ë°°ì¹˜í™”

# í”„ë¡œì íŠ¸ ì»¤ìŠ¤í…€ ëª¨ë“ˆë“¤
from datasets import imagenet_deprocess_batch  # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
import datasets  # VoxCeleb ë°ì´í„°ì…‹ ë¡œë”
import models  # ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì¡° (ìƒì„±ê¸°, íŒë³„ê¸°)
import models.perceptual  # ì¸ì§€ì  ì†ì‹¤ í•¨ìˆ˜ (FaceNet ë“±)
from utils.losses import get_gan_losses  # GAN ì†ì‹¤ í•¨ìˆ˜ (LSGAN, WGAN ë“±)
from utils import timeit, LossManager  # ì‹œê°„ ì¸¡ì •ê³¼ ì†ì‹¤ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
from options.opts import args, options  # ì„¤ì • ì˜µì…˜ë“¤
from utils.logger import Logger  # í…ì„œë³´ë“œ ë¡œê¹…
from utils import tensor2im  # í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
from utils.utils import load_my_state_dict  # ì»¤ìŠ¤í…€ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”©
# losseds need to be modified
from utils.training_utils import add_loss, check_model, calculate_model_losses  # í›ˆë ¨ ìœ í‹¸ë¦¬í‹°
from utils.training_utils import visualize_sample  # ìƒ˜í”Œ ì‹œê°í™”
from utils.evaluate import evaluate  # ëª¨ë¸ í‰ê°€ ë©”íŠ¸ë¦­
from utils.evaluate_fid import evaluate_fid  # FID ì ìˆ˜ ê³„ì‚°
from utils.s2f_evaluator import S2fEvaluator  # Speech-to-Face ì „ìš© í‰ê°€ê¸°

# ê³ ì •ëœ ì…ë ¥ í¬ê¸°ì—ì„œ ë” ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´ cuDNN ë²¤ì¹˜ë§ˆí¬ í™œì„±í™”
torch.backends.cudnn.benchmark = True


def main():
    """
    ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜ - ì „ì²´ í›ˆë ¨ ê³¼ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    í¬í•¨ ë‚´ìš©:
    1. ë°ì´í„° ë¡œë” ì„¤ì •
    2. ëª¨ë¸ êµ¬ì¶• (ìƒì„±ê¸°ì™€ íŒë³„ê¸°ë“¤)
    3. ìµœì í™”ê¸° ì„¤ì •
    4. í›ˆë ¨ ë£¨í”„ì™€ ì†ì‹¤ ê³„ì‚°
    5. ëª¨ë¸ í‰ê°€ì™€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    """
    global args, options
    
    # ë””ë²„ê¹…ì„ ìœ„í•œ ì„¤ì • ì¶œë ¥
    print(args)  # ëª…ë ¹í–‰ ì¸ìë“¤
    print(options['data'])  # ë°ì´í„° ì„¤ì • ì˜µì…˜ë“¤
    
    # GPU ê°€ì†ì„ ìœ„í•œ CUDA ë°ì´í„° íƒ€ì… ì„¤ì •
    # FloatTensor: ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™” ê°’ìš©
    # LongTensor: ì •ìˆ˜ ë ˆì´ë¸”ê³¼ ì¸ë±ìŠ¤ìš©
    float_dtype = torch.cuda.FloatTensor
    long_dtype = torch.cuda.LongTensor
    
    # í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” êµ¬ì¶•
    log.info("Building loader...")
    train_loader, val_loader, test_loader = \
        datasets.build_loaders(options["data"])
        
    # Fuser ì „ìš© í›ˆë ¨ ëª¨ë“œë¥¼ ìœ„í•œ íŠ¹ë³„ ì„¤ì •
    # FuserëŠ” ìŒì„±ê³¼ ì‹œê°ì  íŠ¹ì§•ì„ ê²°í•©í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤
    if args.train_fuser_only:
        # ë” ê°„ë‹¨í•œ ë°°ì¹˜í™”ë¥¼ ìœ„í•´ ê¸°ë³¸ collate í•¨ìˆ˜ ì‚¬ìš©
        train_loader.collate_fn = default_collate
        # ì „ì²´ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ëŒ€ì‹  ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì„¸ê·¸ë¨¼íŠ¸ ë°˜í™˜
        train_loader.dataset.return_mel_segments = True
        # ë°ì´í„° ì¦ê°•ì„ ìœ„í•´ ë©œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¬´ì‘ìœ„ ì‹œì‘ì ì—ì„œ ì¶”ì¶œ
        train_loader.dataset.mel_segments_rand_start = True
        val_loader.collate_fn = default_collate
        val_loader.dataset.return_mel_segments = True
        # Fuser í›ˆë ¨ìš© ë‹¨ìˆœí•œ ì–¼êµ´ ìƒì„± ëª¨ë“œ
        s2f_face_gen_mode = 'naive'
    else:
        # ì¼ë°˜ í›ˆë ¨ì—ì„œëŠ” í‰ê· í™”ëœ FaceNet ì„ë² ë”© ì‚¬ìš©
        s2f_face_gen_mode = 'average_facenet_embedding'
    
    # ê²€ì¦ì„ ìœ„í•œ ë°ì´í„°ì…‹ í¬ê¸° ë¡œê¹…
    log.info("Dataset sizes - Train: {}, Val: {}, Test: {}".format(
        len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)))
    
    # Speech-to-Face ì „ìš© í‰ê°€ê¸° ì´ˆê¸°í™”
    # ì´ í‰ê°€ê¸°ëŠ” recall, ì½”ì‚¬ì¸ ìœ ì‚¬ë„, L1 ê±°ë¦¬ ë“±ì˜ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤
    s2f_val_evaluator = S2fEvaluator(
        val_loader,
        options,
        extraction_size=[100,200,300],  # í‰ê°€ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë“¤
        hq_emb_dict=True,  # ê³ í’ˆì§ˆ ì„ë² ë”© ì‚¬ìš©
        face_gen_mode=s2f_face_gen_mode)
        
    # í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ê³ ìœ í•œ ì‚¬ëŒ ìˆ˜ ê³„ì‚°
    # ì´ëŠ” íŒë³„ê¸°ì—ì„œ ì‹ ì› ë¶„ë¥˜ë¥¼ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤
    num_train_id = len(train_loader.dataset)
    
    # ì‹ ì› ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬ë“¤ì— ì˜¬ë°”ë¥¸ ì‹ ì› ê°œìˆ˜ë¡œ ì´ˆê¸°í™”
    # ì´ë“¤ì€ ìƒì„±ëœ ì–¼êµ´ì˜ ì‹ ì› ë³´ì¡´ì„ ë•ëŠ” ë³´ì¡° ë¶„ë¥˜ê¸°ë“¤ì…ë‹ˆë‹¤
    for ac_net in ['identity', 'identity_low', 'identity_mid', 'identity_high']:
        if options['discriminator'].get(ac_net) is not None:
            options['discriminator'][ac_net]['num_id'] = num_train_id

    # ë©”ì¸ ìƒì„± ëª¨ë¸ (Generator) êµ¬ì¶•
    # ì´ ëª¨ë¸ì€ ìŒì„± íŠ¹ì§•ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
    log.info("Building Generative Model...")
    model, model_kwargs = models.build_model(
        options["generator"],  # ìƒì„±ê¸° êµ¬ì¡° ì„¤ì •
        image_size=options["data"]["image_size"],  # ì¶œë ¥ ì´ë¯¸ì§€ í•´ìƒë„
        checkpoint_start_from=args.checkpoint_start_from)  # ì„ íƒì  ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜
        
    # ëª¨ë¸ì„ GPUë¡œ ì´ë™í•˜ê³  ë°ì´í„° íƒ€ì… ì„¤ì •
    model.type(float_dtype)
    
    # Fuser ì»´í¬ë„ŒíŠ¸ë§Œ í›ˆë ¨í•˜ëŠ” íŠ¹ë³„ ëª¨ë“œ
    if args.train_fuser_only:
        # BatchNorm í†µê³„ë¥¼ ê³ ì •í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        # ì´ëŠ” Fuser í›ˆë ¨ ì¤‘ BatchNorm ë ˆì´ì–´ê°€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤
        model.eval()
        if args.train_fuser_decoder:
            # ì¸ì½”ë”ì˜ Fuser ë¶€ë¶„ë§Œ í›ˆë ¨
            model.encoder.train_fuser_only()
        else:
            # Fuser ì»´í¬ë„ŒíŠ¸ë§Œ í›ˆë ¨
            model.train_fuser_only()
    
    # ë””ë²„ê¹…ì„ ìœ„í•œ ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
    print(model)

    # ìƒì„±ê¸°ë¥¼ ìœ„í•œ ìµœì í™”ê¸° ì„¤ì •
    # Adam ìµœì í™”ê¸°ì™€ í•™ìŠµë¥ , ë² íƒ€ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©
    # filter()ëŠ” í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë§Œ ìµœì í™”í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤
    optimizer = torch.optim.Adam(
            filter(lambda x: x.requires_grad, model.parameters()),
            lr=args.learning_rate,  # í•™ìŠµë¥  (ë³´í†µ 0.0001-0.001)
            betas=(args.beta1, 0.999),)  # Adamì˜ ëª¨ë©˜í…€ ë§¤ê°œë³€ìˆ˜

    # ì´ë¯¸ì§€ íŒë³„ê¸°(ë“¤) êµ¬ì¶•
    # íŒë³„ê¸°ë“¤ì€ ì‹¤ì œì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ êµ¬ë³„í•˜ë ¤ê³  ì‹œë„í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤
    # ì´ë“¤ì€ ìƒì„±ê¸° í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì ëŒ€ì  ì†ì‹¤ì„ ì œê³µí•©ë‹ˆë‹¤
    if (options["optim"]["d_loss_weight"] < 0 or \
        options["optim"]["d_img_weight"] < 0):
        # ê°€ì¤‘ì¹˜ê°€ ìŒìˆ˜ì´ë©´ ì´ë¯¸ì§€ íŒë³„ê¸° ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”ë¨)
        img_discriminator = None
        d_img_kwargs = {}
        log.info("Ignoring Image Discriminator.")
    else:
        # ì´ë¯¸ì§€ íŒë³„ê¸° ë„¤íŠ¸ì›Œí¬(ë“¤) êµ¬ì¶•
        img_discriminator, d_img_kwargs = models.build_img_discriminator(
            options["discriminator"])
        log.info("Done Building Image Discriminator.")

    # ë³´ì¡° ë¶„ë¥˜ê¸° (AC) íŒë³„ê¸°(ë“¤) êµ¬ì¶•  
    # ì´ íŒë³„ê¸°ë“¤ì€ ë˜í•œ ì‚¬ëŒì˜ ì‹ ì›ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤
    # ì´ëŠ” ìƒì„±ëœ ì–¼êµ´ì—ì„œ ì‹ ì› ì¼ê´€ì„±ì„ ë³´ì¡´í•˜ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤
    if (options["optim"]["d_loss_weight"] < 0 or \
        options["optim"]["ac_loss_weight"] < 0):
        # ê°€ì¤‘ì¹˜ê°€ ìŒìˆ˜ì´ë©´ AC íŒë³„ê¸° ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”ë¨)
        ac_discriminator = None
        ac_img_kwargs = {}
        log.info("Ignoring Auxilary Classifier Discriminator.")
    else:
        # ë³´ì¡° ë¶„ë¥˜ê¸° íŒë³„ê¸° ë„¤íŠ¸ì›Œí¬(ë“¤) êµ¬ì¶•
        ac_discriminator, ac_img_kwargs = models.build_ac_discriminator(
            options["discriminator"])
        log.info("Done Building Auxilary Classifier Discriminator.")

    # ì¡°ê±´ë¶€ íŒë³„ê¸°(ë“¤) êµ¬ì¶•
    # ì´ íŒë³„ê¸°ë“¤ì€ ì¶”ê°€ì ì¸ ì¡°ê±´ ì •ë³´ë¥¼ ë°›ìŠµë‹ˆë‹¤
    # ì˜ˆë¥¼ ë“¤ì–´, ìŒì„± íŠ¹ì§•ì„ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    if (options["optim"]["d_loss_weight"] < 0 or \
        options["optim"].get("cond_loss_weight", -1) < 0):
        # ê°€ì¤‘ì¹˜ê°€ ìŒìˆ˜ì´ë©´ ì¡°ê±´ë¶€ íŒë³„ê¸° ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”ë¨)
        cond_discriminator = None
        cond_d_kwargs = {}
        log.info("Ignoring Conditional Discriminator.")
    else:
        # ì¡°ê±´ë¶€ íŒë³„ê¸° ë„¤íŠ¸ì›Œí¬(ë“¤) êµ¬ì¶•
        cond_discriminator, cond_d_kwargs = models.build_cond_discriminator(
            options["discriminator"])
        log.info("Done Building Conditional Discriminator.")

    # ì¸ì§€ì  ì†ì‹¤ ëª¨ë“ˆ êµ¬ì¶•
    # ì¸ì§€ì  ì†ì‹¤ì€ ì›ì‹œ í”½ì…€ ëŒ€ì‹  ê³ ìˆ˜ì¤€ íŠ¹ì§•ì„ ë¹„êµí•©ë‹ˆë‹¤
    # ì´ëŠ” ì¢…ì¢… ë” ì‹œê°ì ìœ¼ë¡œ ë§¤ë ¥ì ì¸ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤
    perceptual_module = None
    if options["optim"].get("perceptual_loss_weight", -1) > 0:
        # ì¸ì§€ì  ì†ì‹¤ êµ¬ì¡° ì´ë¦„ ì–»ê¸° (ì˜ˆ: "FaceNetLoss")
        ploss_name = options.get("perceptual", {}).get("arch", "FaceNetLoss")
        # ì½”ì‚¬ì¸ ì¸ì§€ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ ì–»ê¸°
        ploss_cos_weight = options["optim"].get("cos_percept_loss_weight", -1)
        # ì¸ì§€ì  ì†ì‹¤ ëª¨ë“ˆ êµ¬ì¶•
        perceptual_module = getattr(
            models.perceptual,
            ploss_name)(cos_loss_weight=ploss_cos_weight)
        log.info("Done Building Perceptual {} Module.".format(ploss_name))
        if ploss_cos_weight > 0:
            log.info("Perceptual Cos Loss Weight: {}".format(ploss_cos_weight))
    else:
        log.info("Ignoring Perceptual Module.")

    # GAN ì†ì‹¤ í•¨ìˆ˜ ì–»ê¸°
    # ì´ë“¤ì€ ìƒì„±ê¸°ì™€ íŒë³„ê¸°ê°€ ì–´ë–»ê²Œ ê²½ìŸí•˜ëŠ”ì§€ ì •ì˜í•©ë‹ˆë‹¤
    # ì¼ë°˜ì ì¸ íƒ€ì…ë“¤: LSGAN, WGAN, vanilla GAN
    gan_g_loss, gan_d_loss = get_gan_losses(options["optim"]["gan_loss_type"])

    # ì´ë¯¸ì§€ íŒë³„ê¸°ë“¤ì„ ìœ„í•œ ìµœì í™”ê¸° ì„¤ì •
    # ê° íŒë³„ê¸°ëŠ” ë…ë¦½ì ì¸ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ìì²´ ìµœì í™”ê¸°ê°€ í•„ìš”í•©ë‹ˆë‹¤
    optimizer_d_img = []
    if img_discriminator is not None:
        for i in range(len(img_discriminator)):
            # ê° íŒë³„ê¸°ë¥¼ GPUë¡œ ì´ë™í•˜ê³  í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
            img_discriminator[i].type(float_dtype)
            img_discriminator[i].train()
            print(img_discriminator[i])  # ë””ë²„ê¹…ì„ ìœ„í•œ êµ¬ì¡° ì¶œë ¥
            
            # ê° ì´ë¯¸ì§€ íŒë³„ê¸°ë¥¼ ìœ„í•œ Adam ìµœì í™”ê¸° ìƒì„±
            # íŒë³„ê¸°ë“¤ì€ ë³´í†µ ìƒì„±ê¸°ì™€ ê°™ì€ í•™ìŠµë¥ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤
            optimizer_d_img.append(torch.optim.Adam(
                    filter(lambda x: x.requires_grad,
                           img_discriminator[i].parameters()),
                    lr=args.learning_rate,  # ìƒì„±ê¸°ì™€ ê°™ì€ í•™ìŠµë¥ 
                    betas=(args.beta1, 0.999),))  # ê°™ì€ ë² íƒ€ ë§¤ê°œë³€ìˆ˜

    # ë³´ì¡° ë¶„ë¥˜ê¸° íŒë³„ê¸°ë“¤ì„ ìœ„í•œ ìµœì í™”ê¸° ì„¤ì •
    optimizer_d_ac = []
    if ac_discriminator is not None:
        for i in range(len(ac_discriminator)):
            # ê° AC íŒë³„ê¸°ë¥¼ GPUë¡œ ì´ë™í•˜ê³  í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
            ac_discriminator[i].type(float_dtype)
            ac_discriminator[i].train()
            print(ac_discriminator[i])  # ë””ë²„ê¹…ì„ ìœ„í•œ êµ¬ì¡° ì¶œë ¥
            
            # ê° AC íŒë³„ê¸°ë¥¼ ìœ„í•œ Adam ìµœì í™”ê¸° ìƒì„±
            optimizer_d_ac.append(torch.optim.Adam(
                    filter(lambda x: x.requires_grad,
                           ac_discriminator[i].parameters()),
                    lr=args.learning_rate,  # ìƒì„±ê¸°ì™€ ê°™ì€ í•™ìŠµë¥ 
                    betas=(args.beta1, 0.999),))  # ê°™ì€ ë² íƒ€ ë§¤ê°œë³€ìˆ˜

    # ì¡°ê±´ë¶€ íŒë³„ê¸°ë“¤ì„ ìœ„í•œ ìµœì í™”ê¸° ì„¤ì •
    optimizer_cond_d = []
    if cond_discriminator is not None:
        for i in range(len(cond_discriminator)):
            # ê° ì¡°ê±´ë¶€ íŒë³„ê¸°ë¥¼ GPUë¡œ ì´ë™í•˜ê³  í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
            cond_discriminator[i].type(float_dtype)
            cond_discriminator[i].train()
            print(cond_discriminator[i])  # ë””ë²„ê¹…ì„ ìœ„í•œ êµ¬ì¡° ì¶œë ¥
            
            # ê° ì¡°ê±´ë¶€ íŒë³„ê¸°ë¥¼ ìœ„í•œ Adam ìµœì í™”ê¸° ìƒì„±
            optimizer_cond_d.append(torch.optim.Adam(
                    filter(lambda x: x.requires_grad,
                           cond_discriminator[i].parameters()),
                    lr=args.learning_rate,  # ìƒì„±ê¸°ì™€ ê°™ì€ í•™ìŠµë¥ 
                    betas=(args.beta1, 0.999),))  # ê°™ì€ ë² íƒ€ ë§¤ê°œë³€ìˆ˜

    # ì²´í¬í¬ì¸íŠ¸ ë³µì› ì„¤ì •
    # ì²´í¬í¬ì¸íŠ¸ëŠ” ì´ì „ ìƒíƒœì—ì„œ í›ˆë ¨ì„ ì¬ê°œí•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤
    restore_path = None
    if args.resume is not None:
        # ì¬ê°œ ì¸ìë¡œë¶€í„° ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
        restore_path = '%s_with_model.pt' % args.checkpoint_name
        restore_path = os.path.join(
            options["logs"]["output_dir"], args.resume, restore_path)

    # ì²´í¬í¬ì¸íŠ¸ê°€ ì¡´ì¬í•˜ë©´ ë¡œë“œ
    if restore_path is not None and os.path.isfile(restore_path):
        log.info('Restoring from checkpoint: {}'.format(restore_path))
        # ì „ì²´ ì²´í¬í¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ
        checkpoint = torch.load(restore_path)
        
        # ìƒì„±ê¸° ëª¨ë¸ ìƒíƒœ ë³µì›
        model.load_state_dict(checkpoint['model_state'])
        # ìƒì„±ê¸° ìµœì í™”ê¸° ìƒíƒœ ë³µì› (í•™ìŠµë¥ , ëª¨ë©˜í…€ ë“±)
        optimizer.load_state_dict(checkpoint['optim_state'])

        # ì´ë¯¸ì§€ íŒë³„ê¸° ìƒíƒœë“¤ì´ ì¡´ì¬í•˜ë©´ ë³µì›
        if img_discriminator is not None:
            for i in range(len(img_discriminator)):
                # íŒë³„ê¸° ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³µì›
                term_name = 'd_img_state_%d' % i
                img_discriminator[i].load_state_dict(checkpoint[term_name])
                # íŒë³„ê¸° ìµœì í™”ê¸° ìƒíƒœ ë³µì›
                term_name = 'd_img_optim_state_%d' % i
                optimizer_d_img[i].load_state_dict(checkpoint[term_name])

        # ë³´ì¡° ë¶„ë¥˜ê¸° íŒë³„ê¸° ìƒíƒœë“¤ì´ ì¡´ì¬í•˜ë©´ ë³µì›
        if ac_discriminator is not None:
            for i in range(len(ac_discriminator)):
                # AC íŒë³„ê¸° ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³µì›
                term_name = 'd_ac_state_%d' % i
                ac_discriminator[i].load_state_dict(checkpoint[term_name])
                # AC íŒë³„ê¸° ìµœì í™”ê¸° ìƒíƒœ ë³µì›
                term_name = 'd_ac_optim_state_%d' % i
                optimizer_d_ac[i].load_state_dict(checkpoint[term_name])

        # ì¡°ê±´ë¶€ íŒë³„ê¸° ìƒíƒœë“¤ì´ ì¡´ì¬í•˜ë©´ ë³µì›
        if cond_discriminator is not None:
            for i in range(len(cond_discriminator)):
                # ì¡°ê±´ë¶€ íŒë³„ê¸° ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³µì›
                term_name = 'cond_d_state_%d' % i
                cond_discriminator[i].load_state_dict(checkpoint[term_name])
                # ì¡°ê±´ë¶€ íŒë³„ê¸° ìµœì í™”ê¸° ìƒíƒœ ë³µì›
                term_name = 'cond_d_optim_state_%d' % i
                optimizer_cond_d[i].load_state_dict(checkpoint[term_name])

        # í›ˆë ¨ ì¹´ìš´í„°ì™€ í†µê³„ ë³µì›
        t = checkpoint['counters']['t'] + 1  # í›ˆë ¨ ìŠ¤í… ì¹´ìš´í„°
        
        # í›ˆë ¨ ì§„í–‰ë„ì— ë”°ë¼ ëª¨ë¸ì„ ì ì ˆí•œ ëª¨ë“œë¡œ ì„¤ì •
        if 0 <= args.eval_mode_after <= t:
            model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (ë“œë¡­ì•„ì›ƒ ì—†ìŒ, ê³ ì •ëœ ë°°ì¹˜ì •ê·œí™”)
        else:
            model.train()  # í›ˆë ¨ ëª¨ë“œ ìœ ì§€
            
        # í›ˆë ¨ ì—í¬í¬ ì¹´ìš´í„° ë³µì›
        start_epoch = checkpoint['counters']['epoch'] + 1
        # ì¬ê°œ ë””ë ‰í† ë¦¬ë¡œ ë¡œê·¸ ê²½ë¡œ ì„¤ì •
        log_path = os.path.join(options["logs"]["output_dir"], args.resume,)
        # í•™ìŠµë¥  ë³µì›
        lr = checkpoint.get('learning_rate', args.learning_rate)
        
        # ëª¨ë¸ ì„ íƒì„ ìœ„í•œ ìµœê³  ë©”íŠ¸ë¦­ ì ìˆ˜ë“¤ ë³µì›
        best_inception = checkpoint["counters"].get("best_inception", (0., 0.))
        best_vfs = checkpoint["counters"].get("best_vfs", (0., 0.))
        best_recall_1 = checkpoint["counters"].get("best_recall_1", 0.)
        best_recall_5 = checkpoint["counters"].get("best_recall_5", 0.)
        best_recall_10 = checkpoint["counters"].get("best_recall_10", 0.)
        best_cos = checkpoint["counters"].get("best_cos", 0.)
        best_L1 = checkpoint["counters"].get("best_L1", 100000.0)
        # ì„¤ì • ì˜µì…˜ë“¤ ë³µì›
        options = checkpoint.get("options", options)
    else:
        # ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì²˜ìŒë¶€í„° í›ˆë ¨ ì´ˆê¸°í™”
        t, start_epoch, best_inception, best_vfs = 0, 0, (0., 0.), (0., 0.)
        best_recall_1, best_recall_5, best_recall_10 = 0.0, 0.0, 0.0
        best_cos, best_L1 = 0.0, 100000.0
        lr = args.learning_rate
        
        # ë¹ˆ ì²´í¬í¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        # ì´ëŠ” ëª¨ë“  í›ˆë ¨ ì§„í–‰ë„ì™€ ëª¨ë¸ ìƒíƒœë¥¼ ì €ì¥í•  ê²ƒì…ë‹ˆë‹¤
        checkpoint = {
            'args': args.__dict__,  # ëª…ë ¹í–‰ ì¸ìë“¤
            'options': options,  # ì„¤ì • ì˜µì…˜ë“¤
            'model_kwargs': model_kwargs,  # ëª¨ë¸ ìƒì„± ì¸ìë“¤
            'd_img_kwargs': d_img_kwargs,  # ì´ë¯¸ì§€ íŒë³„ê¸° ì¸ìë“¤
            'train_losses': defaultdict(list),  # í›ˆë ¨ ì†ì‹¤ íˆìŠ¤í† ë¦¬
            'checkpoint_ts': [],  # ì²´í¬í¬ì¸íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ë“¤
            'train_batch_data': [],  # í›ˆë ¨ ë°°ì¹˜ ë°ì´í„° ìƒ˜í”Œë“¤
            'train_samples': [],  # í›ˆë ¨ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤
            'train_iou': [],  # í›ˆë ¨ IoU ì ìˆ˜ë“¤ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
            'train_inception': [],  # í›ˆë ¨ Inception ì ìˆ˜ë“¤
            'lr': [],  # í•™ìŠµë¥  íˆìŠ¤í† ë¦¬
            'val_batch_data': [],  # ê²€ì¦ ë°°ì¹˜ ë°ì´í„° ìƒ˜í”Œë“¤
            'val_samples': [],  # ê²€ì¦ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤
            'val_losses': defaultdict(list),  # ê²€ì¦ ì†ì‹¤ íˆìŠ¤í† ë¦¬
            'val_iou': [],  # ê²€ì¦ IoU ì ìˆ˜ë“¤
            'val_inception': [],  # ê²€ì¦ Inception ì ìˆ˜ë“¤
            'norm_d': [],  # íŒë³„ê¸° ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ë“¤
            'norm_g': [],  # ìƒì„±ê¸° ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ë“¤
            'counters': {  # í›ˆë ¨ ì§„í–‰ë„ ì¹´ìš´í„°ë“¤
                't': None,  # í›ˆë ¨ ìŠ¤í…
                'epoch': None,  # í˜„ì¬ ì—í¬í¬
                'best_inception': None,  # ìµœê³  Inception ì ìˆ˜
                'best_vfs': None,  # ìµœê³  VGGFace ì ìˆ˜
                'best_recall_1': None,  # ìµœê³  Recall@1 ì ìˆ˜
                'best_recall_5': None,  # ìµœê³  Recall@5 ì ìˆ˜
                'best_recall_10': None,  # ìµœê³  Recall@10 ì ìˆ˜
                'best_cos': None,  # ìµœê³  ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                'best_L1': None,  # ìµœê³  L1 ê±°ë¦¬
            },
            # ëª¨ë¸ê³¼ ìµœì í™”ê¸° ìƒíƒœë“¤ (í›ˆë ¨ ì¤‘ì— ì±„ì›Œì§ˆ ê²ƒ)
            'model_state': None,
            'model_best_state': None,
            'optim_state': None,
            'd_img_state': None,
            'd_img_best_state': None,
            'd_img_optim_state': None,
            'd_ac_state': None,
            'd_ac_optim_state': None,
        }

        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” ê³ ìœ í•œ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_path = os.path.join(
            options["logs"]["output_dir"],
            options["logs"]["name"] + "-" + time.strftime("%Y%m%d-%H%M%S")
        )

    # Fuser ë¡œì§ - ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë”©
    if args.pretrained_path is not None and \
        os.path.isfile(args.pretrained_path):
        # ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
        log.info('Loading Pretrained Model: {}'.format(args.pretrained_path))
        pre_checkpoint = torch.load(args.pretrained_path, weights_only=False)
        # ì»¤ìŠ¤í…€ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ë¡œë”© í•¨ìˆ˜ ì‚¬ìš©
        load_my_state_dict(model, pre_checkpoint['model_state'])

        # ì´ë¯¸ì§€ íŒë³„ê¸° ê°€ì¤‘ì¹˜ ë¡œë“œ (ìµœì í™”ê¸°ëŠ” ì œì™¸)
        if img_discriminator is not None:
            for i in range(len(img_discriminator)):
                term_name = 'd_img_state_%d' % i
                img_discriminator[i].load_state_dict(pre_checkpoint[term_name])

        # ë³´ì¡° ë¶„ë¥˜ê¸° íŒë³„ê¸° ê°€ì¤‘ì¹˜ ë¡œë“œ (ìµœì í™”ê¸°ëŠ” ì œì™¸)
        if ac_discriminator is not None:
            for i in range(len(ac_discriminator)):
                term_name = 'd_ac_state_%d' % i
                ac_discriminator[i].load_state_dict(pre_checkpoint[term_name])
                
    # ë¡œê±° ì´ˆê¸°í™” ë° ì„¤ì • íŒŒì¼ ë³µì‚¬
    logger = Logger(log_path)
    log.info("Logging to: {}".format(log_path))
    # save the current config yaml
    copyfile(args.path_opts,
             os.path.join(log_path, options["logs"]["name"] + '.yaml'))

    model = nn.DataParallel(model.cuda())
    if ac_discriminator is not None:
        for i in range(len(ac_discriminator)):
            ac_discriminator[i] = nn.DataParallel(ac_discriminator[i].cuda())
    if img_discriminator is not None:
        for i in range(len(img_discriminator)):
            img_discriminator[i] = nn.DataParallel(img_discriminator[i].cuda())
    if cond_discriminator is not None:
        for i in range(len(cond_discriminator)):
            cond_discriminator[i] = nn.DataParallel(cond_discriminator[i].cuda())
    perceptual_module = nn.DataParallel(perceptual_module.cuda()) if \
        perceptual_module else None

    if args.evaluate:
        assert args.resume is not None
        if args.evaluate_train:
            log.info("Evaluting the training set.")
            train_mean, train_std, train_vfs_mean, train_vfs_std = \
                evaluate(model, train_loader, options)
            log.info("Inception score: {} ({})".format(train_mean, train_std))
            log.info("VggFace score: {} ({})".format(
                train_vfs_mean, train_vfs_std))
        log.info("Evaluting the validation set.")
        val_mean, val_std, vfs_mean, vfs_std = evaluate(
            model, val_loader, options)
        log.info("Inception score: {} ({})".format(val_mean, val_std))
        log.info("VggFace score: {} ({})".format(vfs_mean, vfs_std))
        fid_score = evaluate_fid(model, val_loader, options)
        log.info("FID score: {}".format(fid_score))
        return 0


    got_best_IS = True
    got_best_VFS = True
    got_best_R1 = True
    got_best_R5 = True
    got_best_R10 = True
    got_best_cos = True
    got_best_L1 = True
    others = None
    
    for epoch in range(start_epoch, args.epochs):
        if epoch >= args.eval_mode_after and model.training:
            log.info('[Epoch {}/{}] switching to eval mode'.format(
                epoch, args.epochs))
            model.eval()
            if epoch == args.eval_mode_after:
                optimizer = optim.Adam(
                        filter(lambda x: x.requires_grad, model.parameters()),
                        lr=lr,
                        betas=(args.beta1, 0.999),)
        # ===================================================================
        # SF2F ë…¼ë¬¸ Section 3.7: Progressive Training Strategy
        # í•µì‹¬ ê¸°ì—¬ 8: ì ì§„ì  ì†ì‹¤ í•¨ìˆ˜ ì ìš©ìœ¼ë¡œ ì•ˆì •ì ì¸ í›ˆë ¨
        # ===================================================================
        # íŠ¹ì • ì—í¬í¬ ì´í›„ L1 í”½ì…€ ì†ì‹¤ ë¹„í™œì„±í™”
        # SF2F ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ì ì§„ì  í›ˆë ¨ ì „ëµ: ì´ˆê¸°ì—ëŠ” L1 ì†ì‹¤ë¡œ ê¸°ë³¸ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ê³ 
        # ë‚˜ì¤‘ì—ëŠ” ì ëŒ€ì  ì†ì‹¤ê³¼ ì¸ì§€ì  ì†ì‹¤ì— ì§‘ì¤‘í•˜ì—¬ ë” í˜„ì‹¤ì ì¸ ì–¼êµ´ì„ ìƒì„±
        # ğŸ“ ë…¼ë¬¸ í•µì‹¬: Two-Stage Training Protocol
        # Stage 1: L1 + Adversarial Loss (ê¸°ë³¸ êµ¬ì¡° í•™ìŠµ)
        # Stage 2: Adversarial + Perceptual Loss (ì„¸ë°€í•œ í’ˆì§ˆ í–¥ìƒ)
        if epoch >= args.disable_l1_loss_after and \
            options["optim"]["l1_pixel_loss_weight"] > 1e-10:
            log.info('[Epoch {}/{}] Disable L1 Loss'.format(epoch, args.epochs))
            options["optim"]["l1_pixel_loss_weight"] = 0
            
        # ì—í¬í¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ìš©)
        start_time = time.time()
        
        # í›ˆë ¨ ë°°ì¹˜ ë°˜ë³µ - ì§„í–‰ë¥  í‘œì‹œì¤„ê³¼ í•¨ê»˜
        # pyprind.prog_barëŠ” í˜„ì¬ ì—í¬í¬ì˜ ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ
        for iter, batch in enumerate(pyprind.prog_bar(
            train_loader,
            title="[Epoch {}/{}]".format(epoch, args.epochs),
            width=50)):
            
            # ë°ì´í„° ë¡œë”© ì‹œê°„ ì¸¡ì • (ë³‘ëª© ì§€ì  íŒŒì•…ìš©)
            if args.timing:
                print("Loading Time: {} ms".format(
                    (time.time() - start_time) * 1000))
                    
            # ì „ì—­ í›ˆë ¨ ìŠ¤í… ì¹´ìš´í„° ì¦ê°€
            # ì´ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§, ì‹œê°í™”, ë¡œê¹…ì— ì‚¬ìš©ë©ë‹ˆë‹¤
            t += 1
            
            # ë°°ì¹˜ ë°ì´í„° ì–¸íŒ¨í‚¹ ë° GPUë¡œ ì´ë™
            # SF2Fì˜ í•µì‹¬: ìŒì„±ì—ì„œ ì–¼êµ´ë¡œì˜ ë§¤í•‘ì„ í•™ìŠµ
            imgs, log_mels, human_ids = batch
            # imgs: ì‹¤ì œ ì–¼êµ´ ì´ë¯¸ì§€ (íƒ€ê²Ÿ) - Ground Truth
            imgs = imgs.cuda()
            # log_mels: ë¡œê·¸ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ (ìŒì„± íŠ¹ì§•) - ì…ë ¥
            # SF2F ë…¼ë¬¸ì—ì„œ ìŒì„± íŠ¹ì§•ìœ¼ë¡œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ :
            # 1. ì£¼íŒŒìˆ˜ ì˜ì—­ì—ì„œ ì¸ê°„ì˜ ì²­ê° ì¸ì§€ì™€ ìœ ì‚¬í•œ í‘œí˜„
            # 2. ìŒì„±ì˜ ì¤‘ìš”í•œ íŠ¹ì§•(ìŒë†’ì´, í†¤, ìŒìƒ‰)ì„ íš¨ê³¼ì ìœ¼ë¡œ ìº¡ì²˜
            log_mels = log_mels.type(float_dtype)
            # human_ids: ê° ì‚¬ëŒì˜ ì‹ ì› ë ˆì´ë¸” (ì‹ ì› ë³´ì¡´ì„ ìœ„í•œ ê°ë… ì‹ í˜¸)
            human_ids = human_ids.type(long_dtype)
            
            # ===================================================================
            # SF2F ë…¼ë¬¸ Section 3.1: Speech Fusion to Face Architecture
            # í•µì‹¬ ê¸°ì—¬ 1: ìŒì„± íŠ¹ì§•ì—ì„œ ì–¼êµ´ ì´ë¯¸ì§€ë¡œì˜ ì§ì ‘ì ì¸ ë§¤í•‘ í•™ìŠµ
            # ===================================================================
            # ìƒì„±ê¸° ìˆœì „íŒŒ (Forward Pass)
            # SF2Fì˜ í•µì‹¬ ê³¼ì •: ìŒì„± íŠ¹ì§•ì„ ì–¼êµ´ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            with timeit('forward', args.timing):
                # ğŸ“ ë…¼ë¬¸ í•µì‹¬: log_mels (ë¡œê·¸ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨)ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                # SF2F ë…¼ë¬¸ Figure 2ì—ì„œ ì œì‹œí•œ "Speech Feature Extraction" ë‹¨ê³„
                # - 13ì°¨ì› MFCC ëŒ€ì‹  ë” í’ë¶€í•œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì‚¬ìš©
                # - ì£¼íŒŒìˆ˜ ì˜ì—­ì˜ ì •ë³´ë¡œ ìŒì„±ì˜ í†¤, ìŒë†’ì´, ìŒìƒ‰ íŠ¹ì„± í¬ì°©
                model_out = model(log_mels)
                
                # ğŸ“ ë…¼ë¬¸ í•µì‹¬: ëª¨ë¸ ì¶œë ¥ êµ¬ì¡°
                # imgs_pred: ìƒì„±ëœ ì–¼êµ´ ì´ë¯¸ì§€ë“¤ (ë…¼ë¬¸ì˜ "Generated Face")
                # others: SF2Fì—ì„œ ì œì•ˆí•œ ì¤‘ê°„ í‘œí˜„ë“¤ (Fusion Features, Attention Maps)
                imgs_pred, others = model_out

            # ===================================================================
            # SF2F ë…¼ë¬¸ Section 4.2: Qualitative Results - Visual Monitoring
            # ===================================================================
            # ì£¼ê¸°ì  ì‹œê°í™” - í›ˆë ¨ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
            # SF2F ë…¼ë¬¸ Figure 4ì—ì„œ ë³´ì—¬ì¤€ ì‹œê°ì  í’ˆì§ˆ í‰ê°€ ë°©ë²•ë¡  êµ¬í˜„
            if t % args.visualize_every == 0:
                # í˜„ì¬ í›ˆë ¨ ìƒíƒœ ì €ì¥ í›„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
                training_status = model.training
                model.eval()
                
                # ğŸ“ ë…¼ë¬¸ Figure 4: "Qualitative Comparison" ìƒì„±
                # ì‹¤ì œ ì´ë¯¸ì§€ì™€ ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ë‚˜ë€íˆ ë¹„êµí•˜ì—¬ í’ˆì§ˆ í‰ê°€
                # SF2Fì—ì„œ ê°•ì¡°í•œ "facial similarity" í‰ê°€ì˜ ì‹œê°ì  ê²€ì¦
                samples = visualize_sample(
                    model,
                    imgs,  # Ground Truth ì–¼êµ´ ì´ë¯¸ì§€
                    log_mels,  # ì…ë ¥ ìŒì„± íŠ¹ì§• (SF2Fì˜ í•µì‹¬ ì…ë ¥)
                    options["data"]["data_opts"]["image_normalize_method"],
                    visualize_attn=options['eval'].get('visualize_attn', False))
                
                model.train(mode=training_status)
                logger.image_summary(samples, t, tag="vis")

            # ===================================================================
            # SF2F ë…¼ë¬¸ Section 3.2: Multi-Resolution Training Strategy
            # í•µì‹¬ ê¸°ì—¬ 2: ê³„ì¸µì  ì†ì‹¤ í•¨ìˆ˜ì™€ ë‹¤í•´ìƒë„ í•™ìŠµ
            # ===================================================================
            with timeit('G_loss', args.timing):
                skip_pixel_loss = False

                # ğŸ“ ë…¼ë¬¸ Equation (1): ê¸°ë³¸ ì¬êµ¬ì„± ì†ì‹¤ (Reconstruction Loss)
                # L_rec = ||I_gt - I_gen||_1 (L1 pixel loss)
                # SF2Fì—ì„œ ì œì•ˆí•œ ë‹¤ì¸µ ì†ì‹¤ êµ¬ì¡°ì˜ ê¸°ì´ˆ
                total_loss, losses = calculate_model_losses(
                    options["optim"], skip_pixel_loss, imgs, imgs_pred,)

                # ===================================================================
                # SF2F ë…¼ë¬¸ Section 3.3: Discriminator Training Strategy
                # íŒë³„ê¸°ë“¤ì˜ ë…ë¦½ì ì¸ í›ˆë ¨ìœ¼ë¡œ ì•ˆì •ì ì¸ GAN í•™ìŠµ ë³´ì¥
                # ===================================================================
                with timeit('D_loss', args.timing):
                    # ğŸ“ ë…¼ë¬¸ í•µì‹¬: Multi-Scale Image Discriminator Training
                    # SF2F Figure 2ì˜ "Multi-Scale Discriminator" í›ˆë ¨ ê³¼ì •
                    # ë‹¤ì–‘í•œ í•´ìƒë„ì—ì„œ ì‹¤ì œ/ê°€ì§œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•˜ì—¬ ì„¸ë°€í•œ í’ˆì§ˆ ì œì–´
                    if img_discriminator is not None:
                        d_img_losses = LossManager()
                        for i in range(len(img_discriminator)):
                            # Detach: ìƒì„±ê¸° ê·¸ë˜ë””ì–¸íŠ¸ê°€ íŒë³„ê¸°ë¡œ ì—­ì „íŒŒë˜ì§€ ì•Šë„ë¡ ë°©ì§€
                            # ì´ëŠ” GAN í›ˆë ¨ì˜ ì•ˆì •ì„±ì„ ìœ„í•œ í‘œì¤€ ê¸°ë²•
                            if isinstance(imgs_pred, tuple):
                                imgs_fake = imgs_pred[i].detach()
                            else:
                                imgs_fake = imgs_pred.detach()
                            imgs_real = imgs.detach()
                            
                            # í•´ìƒë„ ë§¤ì¹­: ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ìƒì„±ëœ ì´ë¯¸ì§€ í•´ìƒë„ì— ë§ì¶¤
                            while imgs_real.size()[2] != imgs_fake.size()[2]:
                                imgs_real = F.interpolate(
                                    imgs_real, scale_factor=0.5, mode='nearest')

                            # ğŸ“ ë…¼ë¬¸ Equation (2): Discriminator Loss
                            # L_D = E[log(D(I_real))] + E[log(1 - D(G(s)))]
                            # ì‹¤ì œ ì´ë¯¸ì§€ëŠ” 1ë¡œ, ìƒì„±ëœ ì´ë¯¸ì§€ëŠ” 0ìœ¼ë¡œ ë¶„ë¥˜í•˜ë„ë¡ í›ˆë ¨
                            scores_fake = img_discriminator[i](imgs_fake)
                            scores_real = img_discriminator[i](imgs_real)

                            d_img_gan_loss = gan_d_loss(scores_real, scores_fake)
                            d_img_losses.add_loss(
                                d_img_gan_loss, 'd_img_gan_loss_%d' % i)

                        # íŒë³„ê¸°ë“¤ ë…ë¦½ì  ì—…ë°ì´íŠ¸
                        for i in range(len(img_discriminator)):
                            optimizer_d_img[i].zero_grad()
                        d_img_losses.total_loss.backward()
                        for i in range(len(img_discriminator)):
                            optimizer_d_img[i].step()

                    # ğŸ“ ë…¼ë¬¸ Section 3.4: Auxiliary Classifier Discriminator Training
                    # ì‹ ì› ë³´ì¡´ì„ ìœ„í•œ ë³´ì¡° ë¶„ë¥˜ê¸°ì˜ íŒë³„ê¸° ì—­í•  ë™ì‹œ ìˆ˜í–‰
                    if ac_discriminator is not None:
                        d_ac_losses = LossManager()
                        for i in range(len(ac_discriminator)):
                            if isinstance(imgs_pred, tuple):
                                imgs_fake = imgs_pred[i].detach()
                            else:
                                imgs_fake = imgs_pred.detach()

                                imgs_real = imgs.detach()
                                while imgs_real.size()[2] != imgs_fake.size()[2]:
                                    imgs_real = F.interpolate(
                                        imgs_real, scale_factor=0.5, mode='nearest')

                                # ğŸ“ ë…¼ë¬¸ í•µì‹¬: Dual Function of Auxiliary Classifier
                                # 1) ì‹¤ì œ/ê°€ì§œ íŒë³„ (scores_real/fake)
                                # 2) ì‹ ì› ë¶„ë¥˜ (ac_loss_real) - SF2Fì˜ í•µì‹¬ í˜ì‹ 
                                scores_real, ac_loss_real= ac_discriminator[i](
                                    imgs_real, human_ids)
                                scores_fake, ac_loss_fake = ac_discriminator[i](
                                    imgs_fake, human_ids)

                                # íŒë³„ê¸°ë¡œì„œì˜ ì ëŒ€ì  ì†ì‹¤
                                d_ac_gan_loss = gan_d_loss(scores_real, scores_fake)
                                d_ac_losses.add_loss(
                                    d_ac_gan_loss, 'd_ac_gan_loss_%d' % i)
                                # ì‹¤ì œ ì´ë¯¸ì§€ì—ì„œì˜ ì‹ ì› ë¶„ë¥˜ ì†ì‹¤ (ê°ë… í•™ìŠµ)
                                d_ac_losses.add_loss(
                                    ac_loss_real.mean(), 'd_ac_loss_real_%d' % i)

                            for i in range(len(ac_discriminator)):
                                optimizer_d_ac[i].zero_grad()
                            d_ac_losses.total_loss.backward()
                            for i in range(len(ac_discriminator)):
                                optimizer_d_ac[i].step()

                    # ğŸ“ ë…¼ë¬¸ Section 3.5: Conditional Discriminator Training
                    # ìŒì„± ì¡°ê±´ì„ í™œìš©í•œ ì¡°ê±´ë¶€ íŒë³„ í•™ìŠµ
                    if cond_discriminator is not None:
                        cond_d_losses = LossManager()
                        for i in range(len(cond_discriminator)):
                            if isinstance(imgs_pred, tuple):
                                imgs_fake = imgs_pred[i].detach()
                            else:
                                imgs_fake = imgs_pred.detach()
                            imgs_real = imgs.detach()
                            # ìŒì„± ì¡°ê±´ ë²¡í„° (SF2Fì˜ í•µì‹¬: ìŒì„± íŠ¹ì§•ì„ ì¡°ê±´ìœ¼ë¡œ í™œìš©)
                            cond_vecs = others['cond'].detach()
                            
                            while imgs_real.size()[2] != imgs_fake.size()[2]:
                                imgs_real = F.interpolate(
                                    imgs_real, scale_factor=0.5, mode='nearest')

                            # ğŸ“ ë…¼ë¬¸ Equation (4): Conditional Discriminator Loss
                            # L_D_c = E[log(D_c(I_real, s))] + E[log(1 - D_c(G(s), s))]
                            # ì‹¤ì œ ì´ë¯¸ì§€ì™€ ìŒì„±ì´ ë§¤ì¹­ë˜ëŠ”ì§€, ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ìŒì„±ì´ ë§¤ì¹­ë˜ëŠ”ì§€ íŒë³„
                            scores_fake = cond_discriminator[i](
                                imgs_fake, cond_vecs)
                            scores_real = cond_discriminator[i](
                                imgs_real, cond_vecs)

                            cond_d_gan_loss = gan_d_loss(scores_real, scores_fake)
                            cond_d_losses.add_loss(
                                cond_d_gan_loss, 'cond_d_gan_loss_%d' % i)

                        for i in range(len(cond_discriminator)):
                            optimizer_cond_d[i].zero_grad()
                        cond_d_losses.total_loss.backward()
                        for i in range(len(cond_discriminator)):
                            optimizer_cond_d[i].step()

            losses['total_loss'] = total_loss.item()
            if not math.isfinite(losses['total_loss']):
                log.warn('WARNING: Got loss = NaN, not backpropping')
                continue

            optimizer.zero_grad()
            with timeit('backward', args.timing):
                total_loss.backward()
            optimizer.step()

            total_loss_d = None
            ac_loss_real = None
            ac_loss_fake = None
            d_losses = {}

            with timeit('D_loss', args.timing):
                if img_discriminator is not None:
                    d_img_losses = LossManager()
                    for i in range(len(img_discriminator)):
                        if isinstance(imgs_pred, tuple):
                            imgs_fake = imgs_pred[i].detach()
                        else:
                            imgs_fake = imgs_pred.detach()
                        imgs_real = imgs.detach()
                        
                        while imgs_real.size()[2] != imgs_fake.size()[2]:
                            imgs_real = F.interpolate(
                                imgs_real, scale_factor=0.5, mode='nearest')

                        scores_fake = img_discriminator[i](imgs_fake)
                        scores_real = img_discriminator[i](imgs_real)

                        d_img_gan_loss = gan_d_loss(scores_real, scores_fake)
                        d_img_losses.add_loss(
                            d_img_gan_loss, 'd_img_gan_loss_%d' % i)

                    for i in range(len(img_discriminator)):
                        optimizer_d_img[i].zero_grad()
                    d_img_losses.total_loss.backward()
                    for i in range(len(img_discriminator)):
                        optimizer_d_img[i].step()

                if ac_discriminator is not None:
                    d_ac_losses = LossManager()
                    for i in range(len(ac_discriminator)):
                        if isinstance(imgs_pred, tuple):
                            imgs_fake = imgs_pred[i].detach()
                        else:
                            imgs_fake = imgs_pred.detach()

                            imgs_real = imgs.detach()
                            while imgs_real.size()[2] != imgs_fake.size()[2]:
                                imgs_real = F.interpolate(
                                    imgs_real, scale_factor=0.5, mode='nearest')

                            scores_real, ac_loss_real= ac_discriminator[i](
                                imgs_real, human_ids)
                            scores_fake, ac_loss_fake = ac_discriminator[i](
                                imgs_fake, human_ids)

                            d_ac_gan_loss = gan_d_loss(scores_real, scores_fake)
                            d_ac_losses.add_loss(
                                d_ac_gan_loss, 'd_ac_gan_loss_%d' % i)

                        for i in range(len(ac_discriminator)):
                            optimizer_d_ac[i].zero_grad()
                        d_ac_losses.total_loss.backward()
                        for i in range(len(ac_discriminator)):
                            optimizer_d_ac[i].step()

                if cond_discriminator is not None:
                    cond_d_losses = LossManager()
                    for i in range(len(cond_discriminator)):
                        if isinstance(imgs_pred, tuple):
                            imgs_fake = imgs_pred[i].detach()
                        else:
                            imgs_fake = imgs_pred.detach()
                        imgs_real = imgs.detach()
                        cond_vecs = others['cond'].detach()
                        while imgs_real.size()[2] != imgs_fake.size()[2]:
                            imgs_real = F.interpolate(
                                imgs_real, scale_factor=0.5, mode='nearest')

                        scores_fake = cond_discriminator[i](
                            imgs_fake, cond_vecs)
                        scores_real = cond_discriminator[i](
                            imgs_real, cond_vecs)

                        cond_d_gan_loss = gan_d_loss(scores_real, scores_fake)
                        cond_d_losses.add_loss(
                            cond_d_gan_loss, 'cond_d_gan_loss_%d' % i)

                    for i in range(len(cond_discriminator)):
                        optimizer_cond_d[i].zero_grad()
                    cond_d_losses.total_loss.backward()
                    for i in range(len(cond_discriminator)):
                        optimizer_cond_d[i].step()

            # í›ˆë ¨ ì†ì‹¤ í…ì„œë³´ë“œ ë¡œê¹…
            # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ ëª¨ë“  ì†ì‹¤ ê°’ì„ ê¸°ë¡
            # ìƒì„±ê¸° ì†ì‹¤ë“¤ ë¡œê¹…
            for name, val in losses.items():
                logger.scalar_summary("loss/{}".format(name), val, t)
                
            # ì´ë¯¸ì§€ íŒë³„ê¸° ì†ì‹¤ë“¤ ë¡œê¹…
            if img_discriminator is not None:
                for name, val in d_img_losses.items():
                    logger.scalar_summary("d_loss/{}".format(name), val, t)
                    
            # ë³´ì¡° ë¶„ë¥˜ê¸° íŒë³„ê¸° ì†ì‹¤ë“¤ ë¡œê¹…
            if ac_discriminator is not None:
                for name, val in d_ac_losses.items():
                    logger.scalar_summary("d_loss/{}".format(name), val, t)
                    
            # ì¡°ê±´ë¶€ íŒë³„ê¸° ì†ì‹¤ë“¤ ë¡œê¹…
            if cond_discriminator is not None:
                for name, val in cond_d_losses.items():
                    logger.scalar_summary("d_loss/{}".format(name), val, t)
                    
            # ë‹¤ìŒ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œê°„ ì´ˆê¸°í™”
            start_time = time.time()

        # ===================================================================
        # SF2F ë…¼ë¬¸ Section 4: Experimental Results - Comprehensive Evaluation
        # í•µì‹¬ ê¸°ì—¬ 6: ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ í†µí•œ í¬ê´„ì  ì„±ëŠ¥ í‰ê°€
        # ===================================================================
        # ì£¼ê¸°ì  ê²€ì¦ ë° í‰ê°€ (ì—í¬í¬ ë‹¨ìœ„)
        # SF2F ë…¼ë¬¸ì—ì„œ ê°•ì¡°í•œ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•œ í¬ê´„ì  í‰ê°€
        if epoch % args.eval_epochs == 0:
            log.info('[Epoch {}/{}] checking on val'.format(
                epoch, args.epochs)
            )
            
            # ğŸ“ ë…¼ë¬¸ Table 1: Standard Computer Vision Metrics
            # Inception Score (IS): ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ì¸¡ì •
            # VGGFace Score: ì–¼êµ´ íŠ¹í™” í’ˆì§ˆ ì¸¡ì • (SF2Fì—ì„œ ì œì•ˆ)
            val_results = check_model(
                args, options, epoch, val_loader, model)
            val_losses, val_samples, val_inception, val_vfs = val_results
            
            # ===================================================================
            # SF2F ë…¼ë¬¸ Section 4.1: Quantitative Results - Speech-to-Face Metrics
            # í•µì‹¬ ê¸°ì—¬ 7: ìŒì„±-ì–¼êµ´ ë§¤í•‘ ì„±ëŠ¥ì„ ìœ„í•œ ì „ìš© í‰ê°€ ì§€í‘œ
            # ===================================================================
            # ğŸ“ ë…¼ë¬¸ í•µì‹¬: Speech-to-Face íŠ¹í™” ë©”íŠ¸ë¦­ ê³„ì‚°
            # ì´ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ ìƒì„± í‰ê°€ì™€ ë‹¬ë¦¬ ìŒì„±-ì–¼êµ´ ë§¤í•‘ì˜ ì •í™•ì„±ì„ ì§ì ‘ ì¸¡ì •
            val_facenet_L2_dist, val_facenet_L1_dist, val_facenet_cos_sim, \
                val_recall_tuple, val_ih_sim  = \
                    s2f_val_evaluator.get_metrics(
                        model, recall_method='cos_sim', get_ih_sim=True)
                        
            # ğŸ“ ë…¼ë¬¸ Table 2: Recall@K ë©”íŠ¸ë¦­ë“¤ ì–¸íŒ¨í‚¹
            # SF2F ë…¼ë¬¸ì˜ í•µì‹¬ í‰ê°€ ì§€í‘œ: ìŒì„±ì—ì„œ ì˜¬ë°”ë¥¸ ì–¼êµ´ì„ ì°¾ëŠ” ì •í™•ë„
            # - Recall@1: ê°€ì¥ ìœ ì‚¬í•œ ì–¼êµ´ì´ ì •ë‹µì¸ ë¹„ìœ¨ (ì—„ê²©í•œ í‰ê°€)
            # - Recall@5: ìƒìœ„ 5ê°œ ì¤‘ ì •ë‹µì´ ìˆëŠ” ë¹„ìœ¨ (ì‹¤ìš©ì  í‰ê°€)
            # - Recall@10: ìƒìœ„ 10ê°œ ì¤‘ ì •ë‹µì´ ìˆëŠ” ë¹„ìœ¨ (ê´€ëŒ€í•œ í‰ê°€)
            val_recall_at_1, val_recall_at_2, val_recall_at_5, \
                val_recall_at_10, val_recall_at_20, \
                    val_recall_at_50 = val_recall_tuple
                    
            # ===================================================================
            # SF2F ë…¼ë¬¸: Model Selection via Multiple Metrics
            # ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìµœì  ëª¨ë¸ ì„ íƒ ì „ëµ
            # ===================================================================
            # ê° ë©”íŠ¸ë¦­ë³„ë¡œ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆëŠ”ì§€ í™•ì¸
            # SF2Fì—ì„œëŠ” ë‹¨ì¼ ë©”íŠ¸ë¦­ì´ ì•„ë‹Œ ë‹¤ì¤‘ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¢…í•© í‰ê°€
            
            # ğŸ“ ë…¼ë¬¸ Figure 3: Quality Assessment via Inception Score
            # ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ì¸¡ì •
            if val_inception[0] > best_inception[0]:
                got_best_IS = True
                best_inception = val_inception
                
            # ğŸ“ ë…¼ë¬¸ í•µì‹¬: VGGFace Score for Face Quality
            # SF2Fì—ì„œ ì œì•ˆí•œ ì–¼êµ´ íŠ¹í™” í’ˆì§ˆ ì¸¡ì • ë©”íŠ¸ë¦­
            # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ í’ˆì§ˆì´ ì•„ë‹Œ ì–¼êµ´ì˜ ì‹œê°ì  í’ˆì§ˆì— íŠ¹í™”
            if val_vfs[0] > best_vfs[0]:
                got_best_VFS = True
                best_vfs = val_vfs
                
            # ğŸ“ ë…¼ë¬¸ Table 2: Identity Recall Metrics
            # SF2Fì˜ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ: ìŒì„±-ì–¼êµ´ ë§¤í•‘ ì •í™•ë„
            if val_recall_at_1 > best_recall_1:
                got_best_R1 = True
                best_recall_1 = val_recall_at_1
                
            if val_recall_at_5 > best_recall_5:
                got_best_R5 = True
                best_recall_5 = val_recall_at_5
                
            if val_recall_at_10 > best_recall_10:
                got_best_R10 = True
                best_recall_10 = val_recall_at_10
                
            # ğŸ“ ë…¼ë¬¸: Feature Space Similarity Metrics
            # VGGFace íŠ¹ì§• ê³µê°„ì—ì„œì˜ ìœ ì‚¬ì„± ì¸¡ì • - SF2Fì˜ í•µì‹¬ í‰ê°€ ìš”ì†Œ
            if val_facenet_cos_sim > best_cos:
                got_best_cos = True
                best_cos = val_facenet_cos_sim
                
            # L1 ê±°ë¦¬: íŠ¹ì§• ê³µê°„ì—ì„œì˜ ê±°ë¦¬ ì¸¡ì • (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            if val_facenet_L1_dist < best_L1:
                got_best_L1 = True
                best_L1 = val_facenet_L1_dist
                
            # ì²´í¬í¬ì¸íŠ¸ì— ìµœê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ë“¤ ì €ì¥
            checkpoint['counters']['best_inception'] = best_inception
            checkpoint['counters']['best_vfs'] = best_vfs
            checkpoint['val_samples'].append(val_samples)
            
            # ê²€ì¦ ì†ì‹¤ë“¤ì„ ì²´í¬í¬ì¸íŠ¸ì™€ í…ì„œë³´ë“œì— ê¸°ë¡
            for k, v in val_losses.items():
                checkpoint['val_losses'][k].append(v)
                logger.scalar_summary("ckpt/val_{}".format(k), v, epoch)
                
            # SF2F ì „ìš© ë©”íŠ¸ë¦­ë“¤ì„ í…ì„œë³´ë“œì— ë¡œê¹…
            # ì´ë“¤ì€ Speech-to-Face ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
            logger.scalar_summary("ckpt/val_inception", val_inception[0], epoch)
            logger.scalar_summary("ckpt/val_facenet_L2_dist",
                val_facenet_L2_dist, epoch)
            logger.scalar_summary("ckpt/val_facenet_L1_dist",
                val_facenet_L1_dist, epoch)
            logger.scalar_summary("ckpt/val_facenet_cos_sim",
                val_facenet_cos_sim, epoch)
            logger.scalar_summary("ckpt/val_recall_at_1",
                val_recall_at_1, epoch)
            logger.scalar_summary("ckpt/val_recall_at_2",
                val_recall_at_2, epoch)
            logger.scalar_summary("ckpt/val_recall_at_5",
                val_recall_at_5, epoch)
            logger.scalar_summary("ckpt/val_recall_at_10",
                val_recall_at_10, epoch)
            logger.scalar_summary("ckpt/val_recall_at_20",
                val_recall_at_20, epoch)
            logger.scalar_summary("ckpt/val_recall_at_50",
                val_recall_at_50, epoch)
            logger.scalar_summary("ckpt/val_ih_sim",
                val_ih_sim, epoch)
            logger.scalar_summary("ckpt/val_vfs",
                val_vfs[0], epoch)
                
            # ê²€ì¦ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ì„ í…ì„œë³´ë“œì— ë¡œê¹…
            # ì‹œê°ì  í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ëª¨ë‹ˆí„°ë§ ë„êµ¬
            logger.image_summary(val_samples, epoch, tag="ckpt_val")
            
            # ì½˜ì†”ì— ê²€ì¦ ê²°ê³¼ ì¶œë ¥
            # SF2F ë…¼ë¬¸ì—ì„œ ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¬ ê°ì¢… ë©”íŠ¸ë¦­ë“¤ì˜ í˜„ì¬ ê°’ê³¼ ìµœê³  ê°’ ë¹„êµ
            log.info('[Epoch {}/{}] val inception score: {} ({})'.format(
                    epoch, args.epochs, val_inception[0], val_inception[1]))
            log.info('[Epoch {}/{}] best inception scores: {} ({})'.format(
                    epoch, args.epochs, best_inception[0], best_inception[1]))
            log.info('[Epoch {}/{}] val vfs scores: {} ({})'.format(
                    epoch, args.epochs, val_vfs[0], val_vfs[1]))
            log.info('[Epoch {}/{}] best vfs scores: {} ({})'.format(
                    epoch, args.epochs, best_vfs[0], best_vfs[1]))
            log.info('[Epoch {}/{}] val recall at 5: {}, '.format(
                     epoch, args.epochs, val_recall_at_5) + \
                        'best recall at 5: {}'.format(best_recall_5))
            log.info('[Epoch {}/{}] val recall at 10: {}, '.format(
                     epoch, args.epochs, val_recall_at_10) + \
                        'best recall at 10: {}'.format(best_recall_10))
            log.info('[Epoch {}/{}] val cosine similarity: {}, '.format(
                     epoch, args.epochs, val_facenet_cos_sim) + \
                        'best cosine similarity: {}'.format(best_cos))
            log.info('[Epoch {}/{}] val L1 distance: {}, '.format(
                     epoch, args.epochs, val_facenet_L1_dist) + \
                            'best L1 distance: {}'.format(best_L1))

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì„ ìœ„í•œ ëª¨ë¸ ìƒíƒœ ìˆ˜ì§‘
            # DataParallel ë˜í¼ì—ì„œ ì‹¤ì œ ëª¨ë¸ ìƒíƒœ ì¶”ì¶œ
            checkpoint['model_state'] = model.module.state_dict()

            # ì´ë¯¸ì§€ íŒë³„ê¸° ìƒíƒœë“¤ ì €ì¥
            if img_discriminator is not None:
                for i in range(len(img_discriminator)):
                    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
                    term_name = 'd_img_state_%d' % i
                    checkpoint[term_name] = \
                        img_discriminator[i].module.state_dict()
                    # ìµœì í™”ê¸° ìƒíƒœ ì €ì¥
                    term_name = 'd_img_optim_state_%d' % i
                    checkpoint[term_name] = \
                        optimizer_d_img[i].state_dict()

            # ë³´ì¡° ë¶„ë¥˜ê¸° íŒë³„ê¸° ìƒíƒœë“¤ ì €ì¥
            if ac_discriminator is not None:
                for i in range(len(ac_discriminator)):
                    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
                    term_name = 'd_ac_state_%d' % i
                    checkpoint[term_name] = \
                        ac_discriminator[i].module.state_dict()
                    # ìµœì í™”ê¸° ìƒíƒœ ì €ì¥
                    term_name = 'd_ac_optim_state_%d' % i
                    checkpoint[term_name] = \
                        optimizer_d_ac[i].state_dict()

            # ì¡°ê±´ë¶€ íŒë³„ê¸° ìƒíƒœë“¤ ì €ì¥
            if cond_discriminator is not None:
                for i in range(len(cond_discriminator)):
                    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
                    term_name = 'cond_d_state_%d' % i
                    checkpoint[term_name] = \
                        cond_discriminator[i].module.state_dict()
                    # ìµœì í™”ê¸° ìƒíƒœ ì €ì¥
                    term_name = 'cond_d_optim_state_%d' % i
                    checkpoint[term_name] = \
                        optimizer_cond_d[i].state_dict()

            checkpoint['optim_state'] = optimizer.state_dict()
            checkpoint['counters']['epoch'] = epoch
            checkpoint['counters']['t'] = t
            checkpoint['counters']['best_inception'] = best_inception
            checkpoint['counters']['best_vfs'] = best_vfs
            checkpoint['counters']['best_recall_1'] = best_recall_1
            checkpoint['counters']['best_recall_5'] = best_recall_5
            checkpoint['counters']['best_recall_10'] = best_recall_10
            checkpoint['counters']['best_cos'] = best_cos
            checkpoint['counters']['best_L1'] = best_L1
            checkpoint['lr'] = lr
            checkpoint_path = os.path.join(
                log_path,
                '%s_with_model.pt' % args.checkpoint_name)
            log.info('[Epoch {}/{}] Saving checkpoint: {}'.format(
                epoch, args.epochs, checkpoint_path))
            torch.save(checkpoint, checkpoint_path)
            if got_best_IS:
                copyfile(
                    checkpoint_path,
                    os.path.join(log_path, 'best_IS_with_model.pt'))
                got_best_IS = False
            if got_best_VFS:
                copyfile(
                    checkpoint_path,
                    os.path.join(log_path, 'best_VFS_with_model.pt'))
                got_best_VFS = False
            if got_best_R1:
                copyfile(
                    checkpoint_path,
                    os.path.join(log_path, 'best_R1_with_model.pt'))
                got_best_R1 = False
            if got_best_R5:
                copyfile(
                    checkpoint_path,
                    os.path.join(log_path, 'best_R5_with_model.pt'))
                got_best_R5 = False
            if got_best_R10:
                copyfile(
                    checkpoint_path,
                    os.path.join(log_path, 'best_R10_with_model.pt'))
                got_best_R10 = False
            if got_best_L1:
                copyfile(
                    checkpoint_path,
                    os.path.join(log_path, 'best_L1_with_model.pt'))
                got_best_L1 = False
            if got_best_cos:
                copyfile(
                    checkpoint_path,
                    os.path.join(log_path, 'best_cos_with_model.pt'))
                got_best_cos = False

        if epoch > 0 and epoch % 1000 == 0:
            print('Saving checkpoint for Epoch {}.'.format(epoch))
            copyfile(
                checkpoint_path,
                os.path.join(log_path, 'epoch_{}_model.pt'.format(epoch)))
        # Fuser Logic
        elif args.train_fuser_only and epoch > 0 and epoch % 1 == 0:
            print('Saving checkpoint for Epoch {}.'.format(epoch))
            copyfile(
                checkpoint_path,
                os.path.join(log_path, 'epoch_{}_model.pt'.format(epoch)))
        # End of fuser logic

        if epoch >= args.decay_lr_epochs:
            lr_end = args.learning_rate * 1e-3
            decay_frac = (epoch - args.decay_lr_epochs + 1) / \
                (args.epochs - args.decay_lr_epochs + 1e-5)
            lr = args.learning_rate - decay_frac * (args.learning_rate - lr_end)
            for param_group in optimizer.param_groups:
                param_group["lr"] = lr
            if img_discriminator is not None:
                for i in range(len(optimizer_d_img)):
                    for param_group in optimizer_d_img[i].param_groups:
                        param_group["lr"] = lr
                # for param_group in optimizer_d_img.param_groups:
                #     param_group["lr"] = lr
            log.info('[Epoch {}/{}] learning rate: {}'.format(
                epoch+1, args.epochs, lr))

        logger.scalar_summary("ckpt/learning_rate", lr, epoch)

    # Evaluating after the whole training process.
    log.info("Evaluting the validation set.")
    is_mean, is_std, vfs_mean, vfs_std = evaluate(model, val_loader, options)
    log.info("Inception score: {} ({})".format(is_mean, is_std))
    log.info("VggFace score: {} ({})".format(vfs_mean, vfs_std))

if __name__ == '__main__':
    main()
